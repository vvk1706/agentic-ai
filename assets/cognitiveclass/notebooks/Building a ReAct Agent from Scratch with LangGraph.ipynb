{"cells":[{"cell_type":"markdown","id":"78c599e9-102f-4a08-9eb9-26fe94a117da","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"1469a2ad-e185-46aa-a3ea-1ee2cb80d0bd","metadata":{},"outputs":[],"source":["# **Building a ReAct Agent from Scratch with LangGraph**\n","In this lab, you'll explore the ReAct (Reasoning and Acting) agent framework, which combines reasoning and action in language models to solve complex problems. You'll learn how to implement a ReAct agent from scratch using LangChain and LangGraph, starting with simple reasoning patterns and progressing to more complex implementations with tool usage. By the end of the lab, you'll understand how ReAct agents interleave thinking and acting to tackle multi-step problems that require external information or computation.\n"]},{"cell_type":"markdown","id":"c274476c-16af-4cb2-89a0-793358a874d3","metadata":{},"outputs":[],"source":["Estimated time needed: **90** minutes\n"]},{"cell_type":"markdown","id":"5d574916-e267-4332-bc33-0312ddfc8e76","metadata":{},"outputs":[],"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#What-is-a-ReAct-Agent?\">What is a ReAct Agent?</a>\n","        <ol>\n","            <li><a href=\"#Core-Components-of-ReAct\">Core Components of ReAct</a></li>\n","                <ol>\n","                     <li><a href=\"#Reasoning\">Reasoning</a></li>\n","                    <li><a href=\"#Acting\">Acting</a></li>\n","                </ol>\n","        </ol>\n","    </li>\n","    <li><a href=\"#Search-Tool-API-Key-Setup\">Search Tool API Key Setup</a></li>\n","    <li><a href=\"#The-ReAct-Prompt-Pattern\">The ReAct Prompt Pattern</a></li>\n","        <ol>\n","            <li><a href=\"#Structure-of-a-ReAct-Prompt\">Structure of a ReAct Prompt</a></li>\n","        </ol>\n","    <li><a href=\"#Observing-and-Further-Reasoning\">Observing and Further Reasoning</a></li>\n","        <ol>\n","            <li><a href=\"#Repeat-or-Conclude\">Repeat or Conclude</a></li>\n","        </ol>\n","    <li><a href=\"#Let's-Create-a-more-Complex-ReAct-Agent-from-Scratch-with-Tools-using-LangGraph\">Let's Create a more Complex ReAct Agent from Scratch with Tools using LangGraph</a></li>\n","        <ol>\n","            <li><a href=\"#Define-graph-state\">Define graph state</a></li>\n","        </ol>\n","    <li><a href=\"#Define-Tools\">Define Tools</a></li>\n","        <ol>\n","            <li><a href=\"#Define-all-the-Tools-for-the-ReAct-Agent\">Define all the Tools for the ReAct Agent</a></li>\n","            <li><a href=\"#External-Search-Integration\">External Search Integration</a></li>\n","            <li><a href=\"#API-Key-Setup\">API Key Setup</a></li>\n","        </ol>\n","    <li><a href=\"#Define-the-tools\">Define the tools</a></li>\n","    <li><a href=\"#Define-the-model\">Define the model</a></li>\n","    <li><a href=\"#Load-ReAct-Prompt-Template\">Load ReAct Prompt Template</a></li>\n","    <li><a href=\"#Define-nodes-and-edges\">Define nodes and edges</a></li>\n","    <li><a href=\"#Define-the-graph\">Define the graph</a></li>\n","    <li><a href=\"#Use-ReAct-agent\">Use ReAct agent</a></li>\n","    <li><a href=\"#ReAct-Agent-Execution-Summary\">ReAct Agent Execution Summary</a></li>\n","        <ol>\n","            <li><a href=\"#Flow-Overview\">Flow Overview</a></li>\n","                <ol>\n","                    <li><a href=\"#Input\">Input</a></li>\n","                    <li><a href=\"#Step-by-Step-Reasoning\">Step-by-Step-Reasoning</a></li>  \n","                    <li><a href=\"#System-Behavior\">System Behavior</a></li>   \n","                    <li><a href=\"#Exercise---add-a-tool\">Exercise - add a tool</a></li>\n","                </ol>\n","        </ol>\n","    <li><a href=\"#References\">References</a></li>\n","    <li><a href=\"#Authors\">Authors</a></li>\n","  </ol>\n"]},{"cell_type":"markdown","id":"19f68343-c19f-4a89-b569-eac590208d47","metadata":{},"outputs":[],"source":["## Objectives\n","\n","After completing this lab, you will be able to:\n","\n","- Understand the core components of ReAct (Reasoning and Acting) agents.\n","- Implement Chain of Thought reasoning patterns to improve LLM problem-solving.\n","- Create custom tools that expand an agent's capabilities.\n","- Build a complete ReAct agent using the LangGraph framework.\n","- Design effective prompts that guide agents through reasoning and action cycles.\n","- Implement the full reasoning-action-observation loop for multi-step problem solving.\n"]},{"cell_type":"markdown","id":"109b1955-6636-4daa-a5a7-46589dca3827","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"19d4f3a7-afcc-495c-8771-ee23814ddbd8","metadata":{},"outputs":[],"source":["## Setup\n","\n","For this lab, we will be using the following libraries:\n","\n","*   [`langgraph`](https://python.langchain.com/docs/langgraph) for building and running agent workflows\n","*   [`langchain`](https://python.langchain.com/docs/) for creating language model applications and chains  \n","*   [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for integrating with OpenAI's models\n","*   [`langchainhub`](https://github.com/langchain-ai/langchainhub) for accessing premade prompts and components\n","*   [`IPython.display`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html) for rendering markdown and displaying outputs\n","*   [`typing`](https://docs.python.org/3/library/typing.html) for type annotations and improved code readability\n","*   [`duckduckgo-search`](https://github.com/deedy5/duckduckgo_search) for web search capabilities\n","*   [`langchain-community`](https://python.langchain.com/docs/integrations/) for accessing community-created tools and integrations\n"]},{"cell_type":"markdown","id":"25afe9b9-d922-412a-bf42-2fe36261e115","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n","\n","Run the following to install the required libraries:\n"]},{"cell_type":"code","id":"d5d879ad-e12e-4991-a119-a15550e32ef3","metadata":{},"outputs":[],"source":["%%capture\n!pip install langgraph==0.3.34 langchain-openai==0.3.14 langchainhub==0.1.21 langchain==0.3.24 pygraphviz==1.14 langchain-community==0.3.23"]},{"cell_type":"markdown","id":"ff689ad5-cfb1-4308-ada6-0908ff109a42","metadata":{},"outputs":[],"source":["### Importing Required Libraries\n","We will import the other libraries as we proceed. \n"]},{"cell_type":"code","id":"3006fa72-60f0-43df-bb6a-ae61d3699af1","metadata":{},"outputs":[],"source":["from IPython.display import display, Markdown"]},{"cell_type":"markdown","id":"da9f86a6-152f-417b-ad81-206111372ea2","metadata":{},"outputs":[],"source":["## What is a ReAct Agent?\n","\n","ReAct is an agent framework that combines reasoning and acting within large language models (LLMs) to solve complex problems through a structured approach. Unlike standard language models that generate responses in a single pass, ReAct agents follow an iterative process that alternates between thinking and taking actions.\n","\n","### Core Components of ReAct\n","\n","1. **Reasoning** â€“ The agent thinks through the problem, considering what it knows and what it needs to find out.  \n","2. **Acting** â€“ Based on its reasoning, the agent selects and executes an appropriate action or tool.  \n","3. **Observing** â€“ The agent receives feedback based on the action (that is, tool output).  \n","4. **Further Reasoning** â€“ The agent incorporates observations into its reasoning process.  \n","5. **Repeat or Conclude** â€“ The agent either takes another action or concludes with an answer.\n","\n","What makes ReAct powerful is the continuous feedback loop between reasoning and action, allowing the agent to gather information incrementally and adapt its approach as new information becomes available.\n","\n","We can use the following image to illustrate the ReAct framework:\n","\n","![Screenshot 2025-04-17 at 12.51.55â€¯PM.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/d2zMISxLwsKW7FkIftP-gQ/Screenshot%202025-04-17%20at%2012-51-55%E2%80%AFPM.png) \n","\n","When a query is received, it is passed to the LLM, which begins reasoning about how to address it. If the query requires information beyond the LLMâ€™s internal knowledgeâ€”such as real-time data or external resourcesâ€”the LLM identifies this gap and decides to interact with the external environment (represented by the \"Environment\" box in the diagram) to retrieve the missing information.\n","\n","From there, the LLM decides to call a tool (such as a calculator, a web search function, or a database lookup). That tool interacts with the environment, retrieves the required information, and returns it to the LLM. This is shown by the arrows connecting tools and the environment.\n","\n","The LLM then observes the result, reasons again, and may repeat this processâ€”calling additional tools if neededâ€”until it has enough information to conclude with a final answer.\n","\n","In short, the LLM does not simply guessâ€”it thinks, acts, observes, and iterates as needed, making it more reliable, dynamic, and capable.\n"]},{"cell_type":"markdown","id":"31c9b19c-055d-4e0b-b2e8-10cdf911fb83","metadata":{},"outputs":[],"source":["## Reasoning\n","\n","Reasoning is the foundation of ReAct agents. Language models can perform explicit reasoning, and one of the simplest yet most powerful ways to enhance their performance on complex tasks is just to ask them to think step by step.\n","\n","**Chain of Thought (CoT)** prompting was one of the first effective techniques developed for this. The core idea is simple: rather than asking the model for an answer directly, you guide it to reason step by step.\n","\n","CoT prompting isn't about telling the model what to think but showing it how to think. By providing examples that walk through the reasoning process, you're effectively teaching the model to mimic that kind of logical thinking in its own responses.\n","\n","In ReAct agents, this reasoning capability is critical. It allows the agent to plan what information it needs and what tools it should use to obtain that information.\n"]},{"cell_type":"markdown","id":"def177bf-e606-4bfc-903f-2635b781424f","metadata":{},"outputs":[],"source":["Let's create an instance of the GPT-4o-mini model with minimal randomness (temperature = 0.1).\n"]},{"cell_type":"code","id":"873dc1c3-bd8f-4db6-bb84-ab838aeade72","metadata":{},"outputs":[],"source":["from langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nllm = ChatOpenAI(model=\"gpt-4.1-nano\",temperature=0.1)"]},{"cell_type":"markdown","id":"bd7039ba-f899-4310-9f6d-74385860963f","metadata":{},"outputs":[],"source":["# API Disclaimer\n","This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n","### Running Locally\n","If you are running this lab locally, you will need to configure your own API keys. This lab uses the `ChatOpenAI` module from `langchain`. The local configuration is shown below with instructions to use your own **api_key**. **Replace all instances** with the completed module below throughout the lab.\n","\n","<p style='color: red'><b>DO NOT run the following cell if you aren't running locally, it will cause errors.</b>\n"]},{"cell_type":"code","id":"7d1b6fe4-3ead-4c74-ba2b-94fcfd44536f","metadata":{},"outputs":[],"source":["# IGNORE IF YOU ARE NOT RUNNING LOCALLY\nfrom langchain_ibm import ChatOpenAI\n\nopenai_llm = ChatOpenAI(\n    model=\"gpt-4.1-nano\",\n    api_key=\"your openai api key here\",\n    temperature=0.1\n)"]},{"cell_type":"markdown","id":"8b982113-8c3c-45c1-8662-af318c2b37ad","metadata":{},"outputs":[],"source":["Letâ€™s define a message that may require a bit of interpretation.\n","Weâ€™ll use the human prompt: \"What is 1+1 for robots?\"\n","At first glance, the question seems simple and direct, but it can invite deeper reasoning depending on how the model interprets it. In this case, the model responds with the answer â€œ2â€ and provides a brief explanation of why.\n"]},{"cell_type":"code","id":"d3e30f0f-399f-458b-8826-84e5a0b39d84","metadata":{},"outputs":[],"source":["message=HumanMessage(content=\"what is 1+1 for robots\")\nresponse = llm.invoke([message])\nprint(response.content)"]},{"cell_type":"markdown","id":"c73c4dc6-cd51-46b5-ab0c-15463bc187eb","metadata":{},"outputs":[],"source":["Now we define a message that explicitly walks through a reasoning process using CoT prompting. The prompt begins by noting that robots process numbers in binary, then breaks down the steps leading to the binary result of 1 + 1. \n","\n","The model is provided with step-by-step logic and guided by a structured reasoning path before producing the final answer. When this message is passed to the model, itâ€™s expected to complete the thought by providing the binary resultâ€”\"10\"â€”demonstrating how CoT prompting encourages deeper, more logical outputs.\n"]},{"cell_type":"code","id":"25db1363-5165-4552-a11c-fc81cce342ef","metadata":{},"outputs":[],"source":["message = \"\"\"Robots are computers, and computers process numbers in binary. So, let's think step by step.\n\nQuestion: what is 2+3 for robots?\n']]\nStep 1: In binary, the digits are 0 and 1.\nStep 2: 1 + 1 in binary is similar to decimal 1 + 1.\nStep 3: In decimal, 1 + 1 = 2.\nStep 4: In binary, the number 2 is written as 10.\n\nAnswer:\"\"\"\nresponse = llm.invoke([message])\n\nprint(response.content)"]},{"cell_type":"markdown","id":"4d0e3d27-e8e1-4ff4-a9bc-099e4036e1cd","metadata":{},"outputs":[],"source":["In situations where you lack sufficient knowledge to reason through a problem, you can instruct the language model to engage in its own reasoning process. For [zero shot CoT](https://arxiv.org/abs/2201.11903) , by prompting the model with something like **\"Let's think step by step,\"** you encourage it to deliberate before responding. This approach leverages techniques like CoT prompting, which enhances the model's ability to handle complex tasks by guiding it to generate intermediate reasoning steps.\n"]},{"cell_type":"code","id":"b4abbc86-44a4-426e-9ba8-01150e174c02","metadata":{},"outputs":[],"source":["message = \"\"\"Let's think step by step\n\nQuestion: what is 2+3 for robots?\n\n\n\nAnswer:\"\"\"\nresponse = llm.invoke([message])\n\nprint(response.content)"]},{"cell_type":"markdown","id":"e5f9b435-e23b-4d18-87da-c686738e5544","metadata":{},"outputs":[],"source":["We didnâ€™t initially get the exact answer we wanted, but simply by adding a small piece of text to the prompt, we improved the modelâ€™s performance significantly. This is the \"Reasoning\" step in ReAct. Now, letâ€™s move on to the next step.\n"]},{"cell_type":"markdown","id":"76a95f6e-316e-4716-8172-eaf8d35f2541","metadata":{},"outputs":[],"source":["## Acting \n","\n","\n","Acting is the second key component of ReAct. While reasoning enables structured thinking, it's limited by the knowledge contained within the language model. Acting extends this by allowing the agent to retrieve new information or perform computations using external tools.\n","\n","For example, let's consider asking about the current weather in Tokyo. No matter how well the language model is trained, it cannot know the real-time weather in Tokyo at this moment, since it was trained on data from an earlier point in time.\n","\n","This is where tools come in, they allow the agent to break out of its knowledge limitations and access real-time information or specialized capabilities.\n"]},{"cell_type":"code","id":"9482343e-54ca-4f8a-8227-199b7dc45a1e","metadata":{},"outputs":[],"source":["message=HumanMessage(content=\"What's the weather like in Tokyo today?\")\nresponse = llm.invoke([message])\nprint(response.content)"]},{"cell_type":"markdown","id":"a7fc072f-96ab-4608-bf68-588f68425e10","metadata":{},"outputs":[],"source":["This tool connects our agent to Tavily's search API, enabling it to retrieve information from the internet. This is a crucial capability that allows our agent to access up-to-date information beyond its training data, making it much more versatile when answering knowledge-based questions. Pay attention to the **@tool** decorator before defining the function.  \n"]},{"cell_type":"markdown","id":"240ff6f2-2233-4929-b90a-b56ff46e89b2","metadata":{},"outputs":[],"source":["## Search Tool API Key Setup\n","\n","We'll use Tavily search as our external research tool. You can get an API key at https://app.tavily.com/sign-in    \n","\n","**Disclaimer:** Signing up for Tavily provides you with free credits, more than enough for this project's needs. If you require additional credits for further use, please add them at your own discretion.  \n","\n","You need to copy the key from Tavily's API website and paste the key in \" \" in the code snippet below: os.environ[\"TAVILY_API_KEY\"] = \"**enter your Tavily API key here**\"\n"]},{"cell_type":"code","id":"17fc5d90-ff68-4b69-a048-4b6a3696facf","metadata":{},"outputs":[],"source":["from langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain.tools import tool\nimport os\n\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\n\n# Initialize the Tavily search tool\nsearch = TavilySearchResults()\n\n@tool\ndef search_tool(query: str):\n    \"\"\"\n    Search the web for information using Tavily API.\n\n    :param query: The search query string\n    :return: Search results related to the query\n    \"\"\"\n    return search.invoke(query)\n\n# Note: This tool connects our agent to Tavilyâ€™s real-time search API, enabling retrieval\n# of up-to-date web content. Ideal for queries requiring current information.\n"]},{"cell_type":"markdown","id":"de3bb4ca-eccc-4820-b877-c9dff02bb426","metadata":{},"outputs":[],"source":["Let's create a system message that instructs the agent how to format its responses.\n"]},{"cell_type":"code","id":"7ce9826d-756a-415f-be37-471dae79ef42","metadata":{},"outputs":[],"source":["from langchain.agents import initialize_agent, AgentType\n\n# Initialize the agent with the tool\nagent_with_searchtool = initialize_agent(\n    tools=[search_tool],\n    llm=llm,\n    agent=AgentType.OPENAI_FUNCTIONS,  # Enables tool usage\n    verbose=True,\n    handle_parsing_errors=True)"]},{"cell_type":"code","id":"cc2f85d6-196e-4630-a4ba-ab04c649ae06","metadata":{},"outputs":[],"source":["# from langgraph.prebuilt import create_react_agent\n\n# agent = create_react_agent(\n#     model=llm,\n#     tools=[search_tool],\n#     prompt=\"You are a helpful assistant that can search the web to answer questions.\",\n#     debug=True  # Enables verbose output for debugging\n# )"]},{"cell_type":"markdown","id":"c710eb5b-360e-460d-8a59-337b1282af79","metadata":{},"outputs":[],"source":["Let's try the question again.\n"]},{"cell_type":"code","id":"1c4783aa-cbcb-42be-92a1-faf6d7c0efac","metadata":{},"outputs":[],"source":["# Run the agent with a query that should trigger the tool\nresponse = agent_with_searchtool.run(\"What's the weather like in Tokyo today?\")\n\n# Output the response\nresponse"]},{"cell_type":"markdown","id":"3d00928e-d6e0-4705-9ec9-0cd1f8db965c","metadata":{},"outputs":[],"source":["You can see that we get a much better, and real-time, response.\n"]},{"cell_type":"code","id":"e5f55061-77ad-467c-9663-6a73c5f1264a","metadata":{},"outputs":[],"source":["display(Markdown(response))"]},{"cell_type":"markdown","id":"4391d5d9-d617-4b73-a348-82ab62cdb565","metadata":{},"outputs":[],"source":["# The ReAct Prompt Pattern\n","\n","Now that we've sketched the basics of a simple ReAct problem with a basic tool, let's look at the prompt pattern in detail. The ReAct prompt is a crucial component of the ReAct agent framework, designed to guide the language model in producing structured reasoning and action steps.\n","\n","#### Structure of a ReAct Prompt\n","\n","A ReAct prompt typically follows this structure:\n","\n","1. **Available Tools**: List the tools the agent can useing-action cycle.\n","\n","\n","\n","\n","\n","\n","---\n","\n","   \n","   ```python\n","   You have access to the following tools:\n","   1. Search: Search for information on the web.\n","   2. Calculator: Perform mathematical calculations.\n","   ```\n","---\n","\n","2. **Step Instructions**: Define the expected reasoning process.\n","\n","--- \n","   ```python\n","   For each step:\n","   1. Think about what you know and what you need to find out.\n","   2. If you need more information, use a tool.\n","   3. Format tool usage as: Action: tool_name[parameters].\n","   4. After receiving an observation, continue reasoning.\n","   5. When you have enough information, provide your final answer.\n","   ```\n","---\n","\n","\n","3. **Example Demonstration**: Show a complete example of the reasoning-acting cycle.\n","\n","---   \n","   ```python\n","   Question: What is the population of Paris multiplied by 2?\n","   Thought: I need to find the population of Paris first.\n","   Action: Search[population of Paris France].\n","   Observation: Paris has a population of approximately 2.16 million people.\n","   Thought: Now I need to multiply this number by 2.\n","   Action: Calculator[2.16 * 2]\n","   Observation: 4.32\n","   Answer: 4.32 million\n","   ```\n","---\n","   \n","\n","4. **Current Query and Agent Scratchpad**: The actual question and a working area where the agent's thoughts, actions, and observations are recorded as it works through the problem. The scratchpad serves as the agent's memory, accumulating all steps of the reasoning process.\n","\n","---\n","   \n","   ```python\n","   Question: {question}\n","   {agent_scratchpad}\n","   ```\n","---\n"]},{"cell_type":"markdown","id":"f559cdc1-b954-4337-b504-de5adbc91e1d","metadata":{},"outputs":[],"source":["Let's define a function (`my_message`) that takes in a question and creates a prompt for a ReAct agent. We will include some general outputs including how the AI should behave.  Most notably the LLM was told to stop its thoughts. \n"]},{"cell_type":"code","id":"505582c5-43ef-4b60-a5f2-9540872769aa","metadata":{},"outputs":[],"source":["def my_message(question: str) -> str:\n    return f\"\"\"\nYou are a thoughtful AI assistant tasked with solving the following question:\n\n**{question}**\n\nYou will reason through the problem step by step. I will feed your previous thoughts back to you in future turns so that you can reflect and continue.\n\nUse this format for each response:\n\n---\n\n**Thought:** [your reasoning and reflection here]\n\n---\n\nDo not give a final answer yet unless explicitly asked.  \nIf you see your own previous reasoning repeated as input, that means you should **continue expanding or improving** your thinking â€” not finalize.\n\nOnly when I explicitly ask you to provide a final answer, respond with:\n\n**Final Answer:** [your answer]  \n**DONE**\n\"\"\"\n"]},{"cell_type":"markdown","id":"b7d7c27e-469f-483f-a0e8-c85d447afaf5","metadata":{},"outputs":[],"source":["Now, let's select a question to ask our LLM:\n"]},{"cell_type":"code","id":"25d300c9-f43a-4ddd-91ff-872a1fada0cd","metadata":{},"outputs":[],"source":["my_prompt =my_message('Give me a list of warm-weather vacation locations and select the best one based on other factors')\nprint(my_prompt)"]},{"cell_type":"markdown","id":"e5c97fa8-230f-4646-8430-f944e414058c","metadata":{},"outputs":[],"source":["Finally, we need to incorporate  the scratchpad that serves as the agent's memory, accumulating all steps of the reasoning process.\n"]},{"cell_type":"markdown","id":"52918211-942a-4320-b2ef-631e4b34ac79","metadata":{},"outputs":[],"source":["## Observing and Further Reasoning\n","\n","We know that incorporating observation and iterative reasoning can significantly enhance the effectiveness of an LLM. While reasoning alone enables structured thinking, acting gives the model access to real-time data or external knowledge, expanding the range of problems it can solve.\n","\n","However, the true boost in performance comes from the **feedback** loopâ€”observing the result of an action and applying further reasoning based on this new information. This loop allows the LLM to refine its understanding, correct course, and approach the problem with increasing precision. This is the **Repeat or Conclude** in the ReAct framework.\n"]},{"cell_type":"markdown","id":"71f8832e-d813-432f-9274-18df75d238b1","metadata":{},"outputs":[],"source":["### Repeat or Conclude\n","\n","The Repeat or Conclude step captures the essence of the ReAct loop â€” **the idea is that the LLM feeds its own output back into itself to continue reasoning**. In this example, we implement this using a 'for' loop, where the model is called multiple times, each time receiving its own previous output as part of the new input.\n","\n","Each time through the loop, the model produces a new reasoning trace â€” an intermediate thought that helps build toward the final solution. These traces are stored in a variable called **scratch_pad**, which accumulates the modelâ€™s reasoning over time and acts as a running memory.\n","\n","Although in this simplified example, the LLM is just reflecting on its own thoughts, this same loop structure could be extended to include tool use between iterations.\n","\n","Eventually, when the LLM determines that it has nothing further to add, it concludes it by outputing DONE. Here we limit the **max iterations** to four.\n"]},{"cell_type":"code","id":"7e437c40-5f15-4b5f-8680-5475dc55793a","metadata":{},"outputs":[],"source":["scratch_pad = \"\"\nmax_iterations = 4\n\nfor i in range(max_iterations):\n    display(Markdown(f\"### ðŸ” Iteration {i + 1}\"))\n\n    # Initial message formatting\n    if i == 0:\n        user_input = my_message(\"Book me a vacation to a warm destination.\")\n    else:\n        user_input = scratch_pad\n\n    # Run the agent\n    response = agent_with_searchtool.run(user_input)\n\n    # Format and display the output cleanly\n    display(Markdown(f\"**ðŸ“© LLM Response:**\\n\\n```\\n{response.strip()}\\n```\"))\n\n    # Check for stopping condition\n    if \"DONE\" in response:\n        display(Markdown(\"âœ… **Agent signaled DONE. Ending loop.**\"))\n        break\n\n    # Append to scratch pad\n    scratch_pad += f\"\\n\\nIteration {i + 1}:\\n{response.strip()}\"\n"]},{"cell_type":"code","id":"7a06ca3f-4864-4bd5-bc69-f9fced891a7a","metadata":{},"outputs":[],"source":["display(Markdown(scratch_pad))"]},{"cell_type":"markdown","id":"b8d3c678-3e7c-4205-b29c-fdc6281d698d","metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"markdown","id":"3c409847-76b1-4954-96a3-205c79b5d3cb","metadata":{},"outputs":[],"source":["# Let's Create a more Complex ReAct Agent from Scratch with Tools using LangGraph\n"]},{"cell_type":"markdown","id":"3f0126e9-3396-4d8e-a4e3-56a520fd1251","metadata":{},"outputs":[],"source":["If you're building a ReAct-style agent, a simple **for loop** is a great way to get started. It's easy to control, ideal for testing ideas, and allows you to feed the modelâ€™s output back into itself step by step. However, as things get more complexâ€”for example, when it becomes **unclear** whether the output is coming from a tool or is part of the agentâ€™s reasoning processâ€”that simplicity can turn into a limitation.\n","\n","Thatâ€™s where LangGraph comes in. It lets you structure your agent as a flow of reasoning steps, with built-in support for looping, memory, and a cleaner architecture.\n","\n","**In short**: use a 'for loop' while prototyping or learning, and switch to LangGraph when you're ready for something more scalable and maintainable.\n","\n","\n","\n","\n"]},{"cell_type":"code","id":"2e374707-a002-4d76-8c95-cc62e2b5048f","metadata":{},"outputs":[],"source":["import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")"]},{"cell_type":"markdown","id":"ba4d186f-3579-4050-ad16-c3f3072a2d4c","metadata":{},"outputs":[],"source":["## Define graph state \n","We are going to define the custom ReAct state in this example, which will contain the user's query, a list of messages, and the `agent_outcome`.\n","  \n","For your specific use case, feel free to add any other state keys that you need.  \n","\n","The `agent_outcome` field stores the result of the agent's reasoning process. This can be either an `AgentAction` (indicating the agent has decided to perform an action using a tool) or an `AgentFinish` (indicating the agent has arrived at a final answer).\n","\n","Including `agent_outcome` in your state allows you to:\n","\n","- Control Workflow: Determine the next step in your LangGraph workflow based on whether the agent has finished reasoning or needs to perform an action.\n","\n","- Debugging: Inspect the agent's decisions at each step, which is invaluable for debugging and understanding agent behavior.\n","\n","- State Management: Maintain a clear and structured state that reflects the agent's current status.\n","\n","---\n","\n","\n","The `input` field represents the original user query or instruction. Keeping this in the state is useful for:  \n","  \n","- Contextual Awareness: Ensuring that the agent's actions and decisions are always grounded in the original user input.  \n","\n","- Logging and Auditing: Facilitating logging of user interactions for auditing or analytics purposes.  \n"," \n","- Reusability: Allowing different parts of your workflow to access the original input without needing to pass it explicitly.\n","\n","---\n","\n","Last but not least, we need to keep track of the full conversation history between the user, the agent, and any tools that are used. This is done through the messages field.  \n","  \n","The messages field represents the running chat history, including:  \n","\n","- HumanMessage (user input)  \n","  \n","- AIMessage (agent's thoughts and tool calls)  \n","    \n","- ToolMessage (tool responses)  \n","  \n","- SystemMessage (instructions to the LLM) - optionally  \n","  \n","In LangGraph, the messages field is annotated with a **reducer**, in this case, add_messages, to allow it to be updated incrementally and intelligently.  \n","\n","--- \n","\n","**Why use add_messages as a reducer?**  \n","  \n","By default, LangGraph state updates **overwrite** previous values unless you use a **reducer** function. add_messages is a prebuilt smart reducer that allows:\n","\n","- Appending new messages during each step of the agent's execution.  \n","  \n","- Overwriting existing messages with the **same message ID** (e.g., retrying or correcting a tool call).  \n","  \n","- Human-in-the-loop editing, where a user or external process can revise the conversation history without causing duplication.  \n","  \n","This is more powerful and safer than using operator.add, which simply appends new items blindly (potentially duplicating entries).  \n","  \n","Together, these three fields â€” input, agent_outcome, and messages â€” provide a robust foundation for any ReAct-style agent built with LangGraph. \n","  \n"]},{"cell_type":"code","id":"793d0060-ee33-4700-81f9-c4a96743f6c6","metadata":{},"outputs":[],"source":["import operator\nfrom typing import Annotated, TypedDict, Union, Sequence, List, Tuple\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\n\nclass AgentState(TypedDict):\n    \"\"\"\n    This defines the structure of our agent's state:\n    - input: The user's query or instruction\n    - agent_outcome: What the agent decides to do (take an action or provide final answer)\n    - messages: The conversation history\n    - intermediate_steps: A history of (action, observation) pairs used for scratchpad prompting\n    \"\"\"\n    # add_messages is a reducer\n    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n    \n    input: str\n    agent_outcome: Union[AgentAction, AgentFinish, None]\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n\n"]},{"cell_type":"markdown","id":"e205fa13-53aa-4157-92f9-19786f4e2fb1","metadata":{},"outputs":[],"source":["# Define Tools\n"]},{"cell_type":"markdown","id":"a380657f-2136-4340-8282-dabf1c958b11","metadata":{},"outputs":[],"source":["## Define all the Tools for the ReAct Agent\n","\n","We had defined the `search tool` before, but to build a more complex ReAct agent we require  more tools for the agent to choose from during its action phase. Since the ReAct pattern emphasizes reasoning before acting, we need to define these tools upfront so the agent can reason about which tools to use and how to use them effectively.  \n","\n","Here, we also define a `recommend_clothing` tool. The agent will use it when asked to recommend proper clothing for a weather. This is just an example. You can add as many tools as your agent required here. Make sure to include what the tool does inside the docstrings under tool definition so the agent can understand what the tool does and when to use it, for example :  \n","\n","    \"\"\"\n","    Suggest what to wear based on weather conditions.\n","\n","    :param weather: A brief description of the weather (e.g. \"Overcast, 64.9Â°F\")\n","    :return: A recommendation on what to wear\n","    \"\"\"\n","\n"]},{"cell_type":"markdown","id":"c572f4fe-17d9-439c-b872-d77c407d20e0","metadata":{},"outputs":[],"source":["## External Search Integration\n","\n","Let's set up the Tavily search tool that will allow our agent to perform external research.\n","  \n","This code sets up a web search tool using the Tavily API:\n","\n","`TavilySearchAPIWrapper()` creates a basic search interface that:\n","* Makes HTTP requests to Tavily's API endpoints.\n","* Handles authentication using your API key.\n","* Processes raw search results into readable text.\n","* Returns plaintext summaries of search results.\n","\n","The output is a dictionary where:\n","* `url`: Contains the source webpage URL.\n","* `content`: Contains the relevant text excerpt from the webpage.\n"]},{"cell_type":"markdown","id":"1c8258e6-0abf-4fe2-9767-48a905ae1f83","metadata":{},"outputs":[],"source":["## API Key Setup\n","\n","We'll use Tavily search as our external research tool. You can get an API key at https://app.tavily.com/sign-in    \n","\n","**Disclaimer:** Signing up for Tavily provides you with free credits, more than enough for this project's needs. If you require additional credits for further use, please add them at your own discretion.  \n","\n"]},{"cell_type":"code","id":"cc0ba9e6-fe6e-4885-b370-29c75b023e33","metadata":{},"outputs":[],"source":["os.environ[\"TAVILY_API_KEY\"] = \"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\""]},{"cell_type":"markdown","id":"24b1c670-24eb-47aa-8738-98b9b1deb406","metadata":{},"outputs":[],"source":["## Define the tools\n","This function uses a web search engine (like Tavily or DuckDuckGo) to retrieve and return relevant results for a given query string.\n"]},{"cell_type":"code","id":"a6724c47-69f5-4c76-b487-a27d599d77c2","metadata":{},"outputs":[],"source":["# External tools\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain.tools import tool\nimport os\n\nsearch = TavilySearchResults()\n\n# Search Tool\n@tool\ndef search_tool(query: str) -> str:\n    \"\"\"\n    Search the web for information using a search engine like Tavily or DuckDuckGo.\n\n    :param query: The search query string\n    :return: Search results related to the query\n    \"\"\"\n    return search.invoke(query) \n\n\n# Note: This tool connects our agent to Tavilyâ€™s real-time search API, enabling retrieval\n# of up-to-date web content. Ideal for queries requiring current information.\n\n"]},{"cell_type":"markdown","id":"cce74f90-8c56-4b42-bbe7-1458ed6ece24","metadata":{},"outputs":[],"source":["This function analyzes a weather description string and returns a clothing recommendation by checking for specific keywords or temperaturesâ€”for example, if it finds words such as 'snow' or 'freezing', it suggests wearing warm clothing.\n"]},{"cell_type":"code","id":"0cc4403d-356c-42a6-9b00-b20f1770c6a8","metadata":{},"outputs":[],"source":["\n@tool\ndef recommend_clothing(weather: str) -> str:\n    \"\"\"\n    Returns a clothing recommendation based on the provided weather description.\n\n    This function examines the input string for specific keywords or temperature indicators \n    (e.g., \"snow\", \"freezing\", \"rain\", \"85Â°F\") to suggest appropriate attire. It handles \n    common weather conditions like snow, rain, heat, and cold by providing simple and practical \n    clothing advice.\n\n    :param weather: A brief description of the weather (e.g., \"Overcast, 64.9Â°F\")\n    :return: A string with clothing recommendations suitable for the weather\n    \"\"\"\n    weather = weather.lower()\n    if \"snow\" in weather or \"freezing\" in weather:\n        return \"Wear a heavy coat, gloves, and boots.\"\n    elif \"rain\" in weather or \"wet\" in weather:\n        return \"Bring a raincoat and waterproof shoes.\"\n    elif \"hot\" in weather or \"85\" in weather:\n        return \"T-shirt, shorts, and sunscreen recommended.\"\n    elif \"cold\" in weather or \"50\" in weather:\n        return \"Wear a warm jacket or sweater.\"\n    else:\n        return \"A light jacket should be fine.\""]},{"cell_type":"markdown","id":"9d528730-33a7-42a8-9fa4-95b15d09fad2","metadata":{},"outputs":[],"source":["The tools can be added to a list to be used as input into the agent.\n"]},{"cell_type":"code","id":"094fe137-90cd-4d0d-90cc-812b0421e218","metadata":{},"outputs":[],"source":["tools = [search_tool, recommend_clothing]\n"]},{"cell_type":"markdown","id":"018ab058-7e69-4ae5-ac5e-a0d153c4688b","metadata":{},"outputs":[],"source":["The agent will also need the tool names, they are extracted using: \n"]},{"cell_type":"code","id":"e3383694-b511-40ed-9b54-c38dd8df5dfd","metadata":{},"outputs":[],"source":["tools_by_name = {tool.name: tool for tool in tools}\nprint(tools_by_name)"]},{"cell_type":"markdown","id":"07db677b-efcb-41a3-bd9b-b0591bcaea0f","metadata":{},"outputs":[],"source":["## Define the model\n","Next, we will define the tools and model we will use for our graph.   \n","  \n"]},{"cell_type":"code","id":"ba6c47cb-9499-49aa-9269-81b5dddc295a","metadata":{},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4\", temperature=0)\n"]},{"cell_type":"markdown","id":"17cf6a24-8b28-48d1-8576-426e3f2c3b4b","metadata":{},"outputs":[],"source":["## Load ReAct Prompt Template\n"]},{"cell_type":"markdown","id":"7988e56e-d6ab-4a8b-8ba0-5d2086997eb7","metadata":{},"outputs":[],"source":["#### Loading the ReAct Prompt from the Hub\n","\n","Now that we have our tools and language model set up, the next step is to obtain the ReAct prompt template. This prompt guides the language model to follow the Reasoningâ€“Actingâ€“Observing pattern that defines the ReAct approach. You'll notice that it's similar to the prompt we discussed earlier.  \n","\n","The prompt must have input keys:  \n","            - `tools`: contains descriptions and arguments for each tool.  \n","            -`tool_names`: contains all tool names.  \n","            -`agent_scratchpad`: contains previous agent actions and tool outputs as a string.  \n"]},{"cell_type":"code","id":"edc968f8-1266-4b3e-a2af-88da71195286","metadata":{},"outputs":[],"source":["from langchain import hub\n\nreact_prompt = hub.pull(\"hwchase17/react\")\nprint(react_prompt)  # this shows the full react_prompt\n\nprint(\"=\" * 40)\nprint(\"ðŸ“„ ReAct Prompt Template\")\nprint(\"=\" * 40)\nprint(react_prompt.template)\nprint(\"=\" * 40)\n"]},{"cell_type":"markdown","id":"7272a785-8bda-4b68-b259-ef334e9f42ea","metadata":{},"outputs":[],"source":["The `hwchase17/react` prompt is a well-designed template specifically crafted for ReAct agents. It contains the necessary structure to:\n","- Guide the model to think through problems step by step.\n","- Format its actions to invoke the appropriate tools.\n","- Process observations from the tool calls.\n","- Make decisions based on accumulated information.\n"]},{"cell_type":"code","id":"c56f4f0f-06f3-44a0-8322-9d1746bedc9d","metadata":{},"outputs":[],"source":["from langchain import hub\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.runnables import RunnableConfig\n\n# Load ReAct prompt from LangChain Hub\nreact_prompt = hub.pull(\"hwchase17/react\")\n\n\ndef format_intermediate_steps(intermediate_steps):\n    \"\"\"Convert intermediate steps into a string for the prompt scratchpad.\"\"\"\n    scratchpad = \"\"\n    for action, observation in intermediate_steps:\n        scratchpad += f\"Thought: {action.log}\\n\"\n        scratchpad += f\"Action: {action.tool}\\n\"\n        scratchpad += f\"Action Input: {action.tool_input}\\n\"\n        scratchpad += f\"Observation: {observation}\\n\"\n    return scratchpad\n\n\n\ndef call_model(state: AgentState, config: RunnableConfig):\n    # We have to format tool names as a string for the prompt (e.g., \"search_tool, recommend_clothing\")\n    # Get tools from config\n    tools = config[\"configurable\"].get(\"tools\", []) # configurable is an Arbitrary dictionary you define (e.g., tool list)\n    # .get() wonâ€™t raise an error if the key doesnâ€™t exist â€” it just returns a fallback instead, [] in this case. \n    \n\n    # Prepare tool names for prompt\n    tool_names = \", \".join(tool.name for tool in tools)\n    \n    \n    # Format the scratchpad (past reasoning steps)\n    scratchpad = format_intermediate_steps(state.get(\"intermediate_steps\", []))\n\n    # Format the ReAct-style prompt\n    formatted_prompt = react_prompt.format(\n        input=state[\"input\"],\n        tools=tool_names,\n        tool_names=tool_names,\n        agent_scratchpad=scratchpad\n    )\n\n    # Use it as a HumanMessage to feed into the model\n    # giving the LLM a formatted string that includes the question and context â€” \n    # i.e., what the \"user\" might say and how the model should behave in response.\n    prompt_message = HumanMessage(content=formatted_prompt) \n\n    # Call the model with prompt only (message history is embedded in the prompt text)\n    response: AIMessage = model.invoke([prompt_message], config)\n  \n\n    # Determine agent outcome\n    if response.tool_calls:\n        agent_outcome = AgentAction(\n            tool=response.tool_calls[0][\"name\"],\n            tool_input=response.tool_calls[0][\"args\"],\n            log=response.content or \"\",\n        )\n    else:\n        agent_outcome = AgentFinish(\n            return_values={\"output\": response.content},\n            log=\"Agent has determined the final answer.\",\n        )\n        print(\"we ard done\",response.content)\n\n    return {\n        \"messages\": [response],\n        \"agent_outcome\": agent_outcome,\n        \"agent_scratchpad\": scratchpad\n    }\n"]},{"cell_type":"code","id":"82c4676a-ea3b-4923-9c4a-bdf0a5409501","metadata":{},"outputs":[],"source":["    # Define the user query and initial state\nuser_query = \"Whatâ€™s the weather in Zurich, and what should I wear check the temperture?\"\n\nfist_input = {\n    \"input\": user_query,\n    \"messages\": [HumanMessage(content=user_query)],\n    \"intermediate_steps\": []  # this is mandatory for scratchpad to work\n}\n"]},{"cell_type":"code","id":"9a236514-3306-4c7f-b56a-41305219ce61","metadata":{},"outputs":[],"source":["scratchpad = format_intermediate_steps(fist_input.get(\"intermediate_steps\", []))\nscratchpad"]},{"cell_type":"code","id":"a5c46127-4f69-434a-8f74-382cc417b2f9","metadata":{},"outputs":[],"source":["tool_names = \", \".join(tool.name for tool in tools)\ntool_names "]},{"cell_type":"code","id":"16da7290-92a9-4073-8945-16721baebd02","metadata":{},"outputs":[],"source":["formatted_prompt = react_prompt.format(\n        input=fist_input [\"input\"],\n        tools=tool_names,\n        tool_names=tool_names,\n        agent_scratchpad=scratchpad\n    )"]},{"cell_type":"code","id":"cec125bd-119e-4c3d-b482-9c33d531ef18","metadata":{},"outputs":[],"source":["print(formatted_prompt)"]},{"cell_type":"markdown","id":"3326a738-4048-4f0a-b4a7-9dcffe2066a8","metadata":{},"outputs":[],"source":["LangGraph injects the tools into each node's function call, and if the node is calling a model (such as your call_model), it also wraps the model with those tools using .with_config(...) behind the scenes.\n"]},{"cell_type":"code","id":"7a5e3c2e-72b1-402e-b815-32ec3fdae93b","metadata":{},"outputs":[],"source":["model_tool = model.bind_tools(tools)"]},{"cell_type":"code","id":"deba4dcc-6f65-40b5-b97c-63d9e242a291","metadata":{},"outputs":[],"source":["response: AIMessage =model_tool.invoke([HumanMessage(formatted_prompt)])\nresponse"]},{"cell_type":"code","id":"8fd7475c-e0fd-4e51-acc6-566ce628f271","metadata":{},"outputs":[],"source":["response.tool_calls\n"]},{"cell_type":"code","id":"903ed499-2500-4ef7-8cfd-fbc029e327e6","metadata":{},"outputs":[],"source":[" tools"]},{"cell_type":"markdown","id":"d6c6a86b-35c6-4509-a5df-4d189d5744fd","metadata":{},"outputs":[],"source":["## Define nodes and edges  \n","Next, let's define our nodes and edges.  \n","  \n","Each node should:\n","\n","- Preserve and update messages.\n","- Set agent_outcome based on the model's decision (optional). \n","- Keep input untouched.\n","    \n","You might consider adding a new node to produce structured outputs or trigger external actions like advanced calculations, or creating a calendar event. Or perhaps you want to modify how the `call_model` node behaves, or adjust the logic in `should_continue` to decide when tools should be used. With LangGraph, it's easy to tailor this foundational structure to fit your unique workflow.\n"]},{"cell_type":"code","id":"ae645602-3685-4bf0-bb80-a6a0124fd6bf","metadata":{},"outputs":[],"source":["import json\nfrom langchain_core.messages import ToolMessage, SystemMessage\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.messages import AIMessage\n\ntools_by_name = {tool.name: tool for tool in tools}\n\n\n#Define our tool node \n\n\ndef tool_node(state: AgentState):\n    outputs = []\n    new_steps = []\n\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n\n        # Append ToolMessage to message list\n        outputs.append(\n            ToolMessage(\n                content=json.dumps(tool_result),\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n\n        # Add intermediate step\n        new_steps.append((\n            AgentAction(\n                tool=tool_call[\"name\"],\n                tool_input=tool_call[\"args\"],\n                log=state[\"messages\"][-1].content or \"\",\n            ),\n            str(tool_result)\n        ))\n\n    return {\n        \"messages\": outputs,\n        \"intermediate_steps\": new_steps  # ðŸ‘ˆ critical!\n    }\n\n\n\ndef should_continue(state: AgentState) -> str:\n    if isinstance(state[\"agent_outcome\"], AgentFinish):\n        return \"end\"\n    return \"continue\"\n"]},{"cell_type":"markdown","id":"05aadf59-bc04-4a5d-8bfa-c0e3a0620693","metadata":{},"outputs":[],"source":["## Define the graph  \n","Now that we have defined all of our nodes and edges, we can define and compile our graph. Depending on if you have added more nodes or different edges, you will need to edit this to fit your specific use case:\n"]},{"cell_type":"code","id":"a78c0f41-180e-46b4-9216-f3a39b159b76","metadata":{},"outputs":[],"source":["from langgraph.graph import StateGraph, END\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"reason\", call_model)\nworkflow.add_node(\"act\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"reason\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `reason`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"reason\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n    # Finally we pass in a mapping.\n    # The keys are strings, and the values are other nodes.\n    # END is a special node marking that the graph should finish.\n    # What will happen is we will call `should_continue`, and then the output of that\n    # will be matched against the keys in this mapping.\n    # Based on which one it matches, that node will then be called.\n    {\n        # If `act`, then we call the tool node.\n        \"continue\": \"act\",\n        # Otherwise we finish.\n        \"end\": END,\n    },\n)\n\n\n\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"act\", \"reason\")\n\n# Now we can compile and visualize our graph\ngraph = workflow.compile()\n\n"]},{"cell_type":"code","id":"03ff5d9e-dbd4-4be9-a5ec-a9a5d6621938","metadata":{},"outputs":[],"source":["from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass"]},{"cell_type":"markdown","id":"6663cb24-dd53-42ae-b84f-4d902cdb8129","metadata":{},"outputs":[],"source":["## Use ReAct agent  \n","Now that we have created our ReAct agent, let's actually put it to the test!\n"]},{"cell_type":"code","id":"41897b38-5dfa-4b98-bd62-83c60f9a6338","metadata":{},"outputs":[],"source":["# Sample execution of the message-based ReAct agent using LangGraph #working version\nfrom langchain.schema import HumanMessage\n\n\nif __name__ == \"__main__\":\n    print(\"ðŸ§  Initializing ReAct Agent with LangGraph\")\n    \n    \n\n    # Define the user query and initial state\n    user_query = \"Whatâ€™s the weather in Zurich, and what should I wear?\"\n\n    inputs = {\n    \"input\": user_query,\n    \"messages\": [HumanMessage(content=user_query)],\n    \"intermediate_steps\": []  # this is mandatory for scratchpad to work\n}\n\n    # Run the agent graph synchronously\n    final_state = graph.invoke(\n    inputs,\n    config={\"configurable\": {\"tools\": tools}}  #  Pass tools here\n    )\n    #final_state = graph.invoke(inputs)\n\n    # Extract and display the final answer (if available)\n    print(\"\\nâœ… Final Answer:\")\n    finish = final_state.get(\"agent_outcome\")\n    if isinstance(finish, AgentFinish):\n        print(finish.return_values[\"output\"])\n    else:\n        print(\"No final answer was returned.\")\n\n    # Print the full reasoning trace from messages\n    print(\"\\nðŸ”Ž Reasoning Trace:\")\n    for msg in final_state[\"messages\"]:\n        try:\n            msg.pretty_print()\n        except AttributeError:\n            print(msg)\n"]},{"cell_type":"markdown","id":"5f00e513-bb13-42f7-9f3d-2a663c787bf7","metadata":{},"outputs":[],"source":["## ReAct Agent Execution Summary\n","\n","### Flow Overview\n","The agent successfully executed a multi-step reasoning task using LangGraph and ReAct prompting.\n","Below is a breakdown of the workflow:\n","\n","### Input\n","**User query:**  \n","*\"Whatâ€™s the weather in Zurich, and what should I wear?\"*\n","\n","### Step-by-Step Reasoning\n","\n","1. **First tool call:**  \n","   - **Tool:** `search_tool`  \n","   - **Input:** `\"Current weather in Zurich\"`  \n","   - **Observation:** `\"The current weather in Zurich is 10 degrees Celsius with light rain.\"`\n","\n","2. **Second tool call:**  \n","   - **Tool:** `recommend_clothing`  \n","   - **Input:** `\"10 degrees Celsius, light rain\"`  \n","   - **Observation:** `\"For 10 degrees Celsius with light rain, it is recommended to wear a medium-weight jacket, \n","      a sweater, long pants, and waterproof shoes. An umbrella or a raincoat would also be useful.\"`\n","\n","3. **Final reasoning and answer:**\n","   - The agent concluded its thought process.\n","   - **Final Answer:**  \n","     *\"The current weather in Zurich is 10 degrees Celsius with light rain. It is recommended to wear a medium-weight \n","     jacket, a sweater, long pants, and waterproof shoes. An umbrella or a raincoat would also be useful.\"*\n","\n","### System Behavior\n","\n","| Component               | Description                                                                                   | Status |\n","|------------------------|-----------------------------------------------------------------------------------------------|--------|\n","| Input message           | Parsed correctly and injected into initial state                                              | âœ… Passed |\n","| First tool execution    | Weather data retrieved from search tool                                                      | âœ… Passed |\n","| Second tool execution   | Clothing advice generated based on weather                                                   | âœ… Passed |\n","| Scratchpad generation   | Thought, action, input, and observation logs formatted into a coherent history string         | âœ… Passed |\n","| Agent outcome tracking  | `AgentAction` and `AgentFinish` returned and handled appropriately                            | âœ… Passed |\n","| Reasoning trace output  | All intermediate steps and final result printed in readable format                           | âœ… Passed |\n","\n","This confirms a correct and fully functional ReAct implementation using LangGraph\n","with tool chaining and reasoning history tracking. You can also define additional tools such as translation or summarization.\n"]},{"cell_type":"markdown","id":"51360cf5-c0fa-49a5-8bd3-df0bd8c7c4f9","metadata":{},"outputs":[],"source":["### Exercise - add a tool \n","\n","Add a temperature conversion tool to convert from Fahrenheit to Celcius and vice versa. Then test your ReAct agent with the following prompt:  \n","\"Whatâ€™s the weather in Tokyo today, what should I wear, and can you convert the temperature to Fahrenheit?\"  \n"]},{"cell_type":"code","id":"27d809c0-cc7b-4cf9-a323-26ad7e1b361f","metadata":{},"outputs":[],"source":["df = # TODO"]},{"cell_type":"markdown","id":"3131229a-c261-4c08-8c53-b832e005f7df","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the solution</summary>\n","\n","```python\n","\n","@tool\n","def convert_temperature(temperature: float, unit: str) -> str:\n","    \"\"\"\n","    Converts temperature between Celsius and Fahrenheit.\n","    \n","    :param temperature: The numeric temperature to convert.\n","    :param unit: 'C' to convert to Celsius, 'F' to convert to Fahrenheit.\n","    :return: Converted temperature as a string.\n","    \"\"\"\n","    unit = unit.upper()\n","    if unit == \"C\":\n","        converted = (temperature - 32) * 5 / 9\n","        return f\"{temperature}Â°F is {converted:.1f}Â°C\"\n","    elif unit == \"F\":\n","        converted = (temperature * 9 / 5) + 32\n","        return f\"{temperature}Â°C is {converted:.1f}Â°F\"\n","    else:\n","        return \"Invalid unit. Please use 'C' or 'F'.\"\n","\n","tools.append(convert_temperature)\n","tools_by_name[convert_temperature.name] = convert_temperature\n","\n","##\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"41f4fab0-08df-46ca-af05-338689b1cc3d","metadata":{},"outputs":[],"source":["## References \n","\n","- https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\n","\n","- https://www.ibm.com/think/topics/react-agent\n"]},{"cell_type":"markdown","id":"c1965a20-ded5-43d9-8b70-b3919b7e8482","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"e5835317-9c13-4acd-b49e-c446773558a3","metadata":{},"outputs":[],"source":["[Faranak Heidari](https://www.linkedin.com/in/faranakhdr/) is a data scientist and GenAI developer in IBM. \n"]},{"cell_type":"markdown","id":"ae30ee14-4d0d-44ee-81a5-3a7a2f53efe6","metadata":{},"outputs":[],"source":["Kunal Makwana is a software developer in IBM. \n"]},{"cell_type":"markdown","id":"815cdab9-4732-4cd3-9c38-13a1acc2105b","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"121acb9a-b7ff-4715-a050-799e51897d2a","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for the changelog</summary>\n","\n","|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2025-06-24|0.4|Mercedes Schneider|QA pass with edits|\n","|2025-06-24|0.3|Steve Ryan|ID review and format/typo fixes|\n","|2024-02-23|0.2|Elio Di Nino|Update library documentation|\n","|2020-07-17|0.1|Sam|Create lab template|\n","\n","</detials>\n"]},{"cell_type":"markdown","id":"1cb5dbff-99ac-4000-bd54-43d6cf7471c3","metadata":{},"outputs":[],"source":["Copyright Â© IBM Corporation. All rights reserved.\n"]},{"cell_type":"code","id":"987573a2-6204-49b6-a9f2-129ac0b6898c","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"9b7e5cef7486e12ef43928838fe7844077fc76200d514eb7d71f5f56d635076a"},"nbformat":4,"nbformat_minor":4}