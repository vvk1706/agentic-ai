{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a ReAct Agent from Scratch with LangGraph**\n",
    "In this lab, you'll explore the ReAct (Reasoning and Acting) agent framework, which combines reasoning and action in language models to solve complex problems. You'll learn how to implement a ReAct agent from scratch using LangChain and LangGraph, starting with simple reasoning patterns and progressing to more complex implementations with tool usage. By the end of the lab, you'll understand how ReAct agents interleave thinking and acting to tackle multi-step problems that require external information or computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **90** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#What-is-a-ReAct-Agent?\">What is a ReAct Agent?</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Core-Components-of-ReAct\">Core Components of ReAct</a></li>\n",
    "                <ol>\n",
    "                     <li><a href=\"#Reasoning\">Reasoning</a></li>\n",
    "                    <li><a href=\"#Acting\">Acting</a></li>\n",
    "                </ol>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Search-Tool-API-Key-Setup\">Search Tool API Key Setup</a></li>\n",
    "    <li><a href=\"#The-ReAct-Prompt-Pattern\">The ReAct Prompt Pattern</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Structure-of-a-ReAct-Prompt\">Structure of a ReAct Prompt</a></li>\n",
    "        </ol>\n",
    "    <li><a href=\"#Observing-and-Further-Reasoning\">Observing and Further Reasoning</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Repeat-or-Conclude\">Repeat or Conclude</a></li>\n",
    "        </ol>\n",
    "    <li><a href=\"#Let's-Create-a-more-Complex-ReAct-Agent-from-Scratch-with-Tools-using-LangGraph\">Let's Create a more Complex ReAct Agent from Scratch with Tools using LangGraph</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Define-graph-state\">Define graph state</a></li>\n",
    "        </ol>\n",
    "    <li><a href=\"#Define-Tools\">Define Tools</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Define-all-the-Tools-for-the-ReAct-Agent\">Define all the Tools for the ReAct Agent</a></li>\n",
    "            <li><a href=\"#External-Search-Integration\">External Search Integration</a></li>\n",
    "            <li><a href=\"#API-Key-Setup\">API Key Setup</a></li>\n",
    "        </ol>\n",
    "    <li><a href=\"#Define-the-tools\">Define the tools</a></li>\n",
    "    <li><a href=\"#Define-the-model\">Define the model</a></li>\n",
    "    <li><a href=\"#Load-ReAct-Prompt-Template\">Load ReAct Prompt Template</a></li>\n",
    "    <li><a href=\"#Define-nodes-and-edges\">Define nodes and edges</a></li>\n",
    "    <li><a href=\"#Define-the-graph\">Define the graph</a></li>\n",
    "    <li><a href=\"#Use-ReAct-agent\">Use ReAct agent</a></li>\n",
    "    <li><a href=\"#ReAct-Agent-Execution-Summary\">ReAct Agent Execution Summary</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Flow-Overview\">Flow Overview</a></li>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Input\">Input</a></li>\n",
    "                    <li><a href=\"#Step-by-Step-Reasoning\">Step-by-Step-Reasoning</a></li>  \n",
    "                    <li><a href=\"#System-Behavior\">System Behavior</a></li>   \n",
    "                    <li><a href=\"#Exercise---add-a-tool\">Exercise - add a tool</a></li>\n",
    "                </ol>\n",
    "        </ol>\n",
    "    <li><a href=\"#References\">References</a></li>\n",
    "    <li><a href=\"#Authors\">Authors</a></li>\n",
    "  </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of ReAct (Reasoning and Acting) agents.\n",
    "- Implement Chain of Thought reasoning patterns to improve LLM problem-solving.\n",
    "- Create custom tools that expand an agent's capabilities.\n",
    "- Build a complete ReAct agent using the LangGraph framework.\n",
    "- Design effective prompts that guide agents through reasoning and action cycles.\n",
    "- Implement the full reasoning-action-observation loop for multi-step problem solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`langgraph`](https://python.langchain.com/docs/langgraph) for building and running agent workflows\n",
    "*   [`langchain`](https://python.langchain.com/docs/) for creating language model applications and chains  \n",
    "*   [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for integrating with OpenAI's models\n",
    "*   [`langchainhub`](https://github.com/langchain-ai/langchainhub) for accessing premade prompts and components\n",
    "*   [`IPython.display`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html) for rendering markdown and displaying outputs\n",
    "*   [`typing`](https://docs.python.org/3/library/typing.html) for type annotations and improved code readability\n",
    "*   [`duckduckgo-search`](https://github.com/deedy5/duckduckgo_search) for web search capabilities\n",
    "*   [`langchain-community`](https://python.langchain.com/docs/integrations/) for accessing community-created tools and integrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Run the following to install the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langgraph==0.3.34 langchain-openai==0.3.14 langchainhub==0.1.21 langchain==0.3.24 pygraphviz==1.14 langchain-community==0.3.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "We will import the other libraries as we proceed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:24:40) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)\n",
    "\n",
    "from IPython.display import display, Markdown, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a ReAct Agent?\n",
    "\n",
    "ReAct is an agent framework that combines reasoning and acting within large language models (LLMs) to solve complex problems through a structured approach. Unlike standard language models that generate responses in a single pass, ReAct agents follow an iterative process that alternates between thinking and taking actions.\n",
    "\n",
    "### Core Components of ReAct\n",
    "\n",
    "1. **Reasoning** – The agent thinks through the problem, considering what it knows and what it needs to find out.  \n",
    "2. **Acting** – Based on its reasoning, the agent selects and executes an appropriate action or tool.  \n",
    "3. **Observing** – The agent receives feedback based on the action (that is, tool output).  \n",
    "4. **Further Reasoning** – The agent incorporates observations into its reasoning process.  \n",
    "5. **Repeat or Conclude** – The agent either takes another action or concludes with an answer.\n",
    "\n",
    "What makes ReAct powerful is the continuous feedback loop between reasoning and action, allowing the agent to gather information incrementally and adapt its approach as new information becomes available.\n",
    "\n",
    "We can use the following image to illustrate the ReAct framework:\n",
    "\n",
    "![Screenshot 2025-04-17 at 12.51.55 PM.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/d2zMISxLwsKW7FkIftP-gQ/Screenshot%202025-04-17%20at%2012-51-55%E2%80%AFPM.png) \n",
    "\n",
    "When a query is received, it is passed to the LLM, which begins reasoning about how to address it. If the query requires information beyond the LLM’s internal knowledge—such as real-time data or external resources—the LLM identifies this gap and decides to interact with the external environment (represented by the \"Environment\" box in the diagram) to retrieve the missing information.\n",
    "\n",
    "From there, the LLM decides to call a tool (such as a calculator, a web search function, or a database lookup). That tool interacts with the environment, retrieves the required information, and returns it to the LLM. This is shown by the arrows connecting tools and the environment.\n",
    "\n",
    "The LLM then observes the result, reasons again, and may repeat this process—calling additional tools if needed—until it has enough information to conclude with a final answer.\n",
    "\n",
    "In short, the LLM does not simply guess—it thinks, acts, observes, and iterates as needed, making it more reliable, dynamic, and capable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n",
    "Reasoning is the foundation of ReAct agents. Language models can perform explicit reasoning, and one of the simplest yet most powerful ways to enhance their performance on complex tasks is just to ask them to think step by step.\n",
    "\n",
    "**Chain of Thought (CoT)** prompting was one of the first effective techniques developed for this. The core idea is simple: rather than asking the model for an answer directly, you guide it to reason step by step.\n",
    "\n",
    "CoT prompting isn't about telling the model what to think but showing it how to think. By providing examples that walk through the reasoning process, you're effectively teaching the model to mimic that kind of logical thinking in its own responses.\n",
    "\n",
    "In ReAct agents, this reasoning capability is critical. It allows the agent to plan what information it needs and what tools it should use to obtain that information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of the GPT-4o-mini model with minimal randomness (temperature = 0.1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\",temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses the `ChatOpenAI` module from `langchain`. The local configuration is shown below with instructions to use your own **api_key**. **Replace all instances** with the completed module below throughout the lab.\n",
    "\n",
    "<p style='color: red'><b>DO NOT run the following cell if you aren't running locally, it will cause errors.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "from langchain_ibm import ChatOpenAI\n",
    "\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=\"your openai api key here\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define a message that may require a bit of interpretation.\n",
    "We’ll use the human prompt: \"What is 1+1 for robots?\"\n",
    "At first glance, the question seems simple and direct, but it can invite deeper reasoning depending on how the model interprets it. In this case, the model responds with the answer “2” and provides a brief explanation of why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For citizens of Alpha Centauri, the basic arithmetic operation 1 + 1 would still equal 2, assuming they use a similar numerical system as humans. If their numbering system or mathematical conventions differ significantly, the answer might vary, but based on our understanding of mathematics as a universal language, 1 + 1 = 2.\n"
     ]
    }
   ],
   "source": [
    "message=HumanMessage(content=\"what is 1+1 for alpha centauri citizens\")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a message that explicitly walks through a reasoning process using CoT prompting. The prompt begins by noting that robots process numbers in binary, then breaks down the steps leading to the binary result of 1 + 1. \n",
    "\n",
    "The model is provided with step-by-step logic and guided by a structured reasoning path before producing the final answer. When this message is passed to the model, it’s expected to complete the thought by providing the binary result—\"10\"—demonstrating how CoT prompting encourages deeper, more logical outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In binary, 2 is written as 10, and 3 is written as 11.\n",
      "\n",
      "Adding them:\n",
      "\n",
      "  10\n",
      "+ 11\n",
      "____\n",
      "\n",
      "Starting from the right:\n",
      "\n",
      "0 + 1 = 1 (no carry)\n",
      "1 + 1 = 10 in binary, which is 0 with a carry of 1\n",
      "\n",
      "Now, add the carry:\n",
      "\n",
      "1 (carry) + 0 (next digit) = 1\n",
      "\n",
      "Putting it all together, the sum is:\n",
      "\n",
      "  10\n",
      "+ 11\n",
      "____\n",
      "  101\n",
      "\n",
      "So, 2 + 3 in binary is 101.\n",
      "\n",
      "**Answer:** 101\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"Robots are computers, and computers process numbers in binary. So, let's think step by step.\n",
    "\n",
    "Question: what is 2+3 for robots?\n",
    "']]\n",
    "Step 1: In binary, the digits are 0 and 1.\n",
    "Step 2: 1 + 1 in binary is similar to decimal 1 + 1.\n",
    "Step 3: In decimal, 1 + 1 = 2.\n",
    "Step 4: In binary, the number 2 is written as 10.\n",
    "\n",
    "Answer:\"\"\"\n",
    "response = llm.invoke([message])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations where you lack sufficient knowledge to reason through a problem, you can instruct the language model to engage in its own reasoning process. For [zero shot CoT](https://arxiv.org/abs/2201.11903) , by prompting the model with something like **\"Let's think step by step,\"** you encourage it to deliberate before responding. This approach leverages techniques like CoT prompting, which enhances the model's ability to handle complex tasks by guiding it to generate intermediate reasoning steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For robots, which use binary calculations, the numbers 2 and 3 are represented in binary as follows:\n",
      "\n",
      "- 2 in binary is 10\n",
      "- 3 in binary is 11\n",
      "\n",
      "Let's add these binary numbers step by step:\n",
      "\n",
      "```\n",
      "   10\n",
      " + 11\n",
      " ----\n",
      "```\n",
      "\n",
      "Starting from the rightmost bit:\n",
      "\n",
      "1. 0 + 1 = 1 (no carry)\n",
      "2. 1 + 1 = 10 in binary, which is 0 with a carry of 1 to the next higher bit\n",
      "\n",
      "Since there's no higher bit, we add the carry:\n",
      "\n",
      "- Carry 1 to the next position, which becomes 1\n",
      "\n",
      "Putting it all together:\n",
      "\n",
      "```\n",
      "   10\n",
      " + 11\n",
      " ----\n",
      "  101\n",
      "```\n",
      "\n",
      "In binary, 101 equals 5 in decimal.\n",
      "\n",
      "**Answer:** 2 + 3 equals 5 for robots.\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"Robots are computers, compueters are using binary calculations. Let's think step by step\n",
    "\n",
    "Question: what is 2+3 for robots?\n",
    "\n",
    "\n",
    "Answer:\"\"\"\n",
    "response = llm.invoke([message])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn’t initially get the exact answer we wanted, but simply by adding a small piece of text to the prompt, we improved the model’s performance significantly. This is the \"Reasoning\" step in ReAct. Now, let’s move on to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acting \n",
    "\n",
    "\n",
    "Acting is the second key component of ReAct. While reasoning enables structured thinking, it's limited by the knowledge contained within the language model. Acting extends this by allowing the agent to retrieve new information or perform computations using external tools.\n",
    "\n",
    "For example, let's consider asking about the current weather in Tokyo. No matter how well the language model is trained, it cannot know the real-time weather in Tokyo at this moment, since it was trained on data from an earlier point in time.\n",
    "\n",
    "This is where tools come in, they allow the agent to break out of its knowledge limitations and access real-time information or specialized capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to real-time weather data. For the most current weather information in Tokyo, please check a reliable weather website or app.\n"
     ]
    }
   ],
   "source": [
    "message=HumanMessage(content=\"What's the weather like in Tokyo today?\")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool connects our agent to Tavily's search API, enabling it to retrieve information from the internet. This is a crucial capability that allows our agent to access up-to-date information beyond its training data, making it much more versatile when answering knowledge-based questions. Pay attention to the **@tool** decorator before defining the function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Tool API Key Setup\n",
    "\n",
    "We'll use Tavily search as our external research tool. You can get an API key at https://app.tavily.com/sign-in    \n",
    "\n",
    "**Disclaimer:** Signing up for Tavily provides you with free credits, more than enough for this project's needs. If you require additional credits for further use, please add them at your own discretion.  \n",
    "\n",
    "You need to copy the key from Tavily's API website and paste the key in \" \" in the code snippet below: os.environ[\"TAVILY_API_KEY\"] = \"**enter your Tavily API key here**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import tool\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the Tavily search tool\n",
    "search = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def search_tool(query: str):\n",
    "    \"\"\"\n",
    "    Search the web for information using Tavily API.\n",
    "\n",
    "    :param query: The search query string\n",
    "    :return: Search results related to the query\n",
    "    \"\"\"\n",
    "    return search.invoke(query)\n",
    "\n",
    "# Note: This tool connects our agent to Tavily’s real-time search API, enabling retrieval\n",
    "# of up-to-date web content. Ideal for queries requiring current information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a system message that instructs the agent how to format its responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Initialize the agent with the tool\n",
    "agent_with_searchtool = initialize_agent(\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,  # Enables tool usage\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# agent = create_react_agent(\n",
    "#     model=llm,\n",
    "#     tools=[search_tool],\n",
    "#     prompt=\"You are a helpful assistant that can search the web to answer questions.\",\n",
    "#     debug=True  # Enables verbose output for debugging\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the question again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_tool` with `{'query': 'погода в Химках сегодня'}`\n",
      "\n",
      "\n",
      "\u001b[36;1m\u001b[1;3m[{'title': 'Прогноз погоды в Химках, Москва и Московская область, Россия', 'url': 'https://yandex.ru/pogoda/ru/himki', 'content': 'Сейчас в Химках ясно, температура воздуха +17°, ощущается как +15°. Ветер 1,7 м/с, З, влажность 47%, атмосферное давление 744 мм рт. ст. В ближайшие 2 часа осадков не ожидается. Сегодня: +12\\u2060…\\u2060+16\\u2060°, ясно, без осадков, слабый ветер 1\\u2060–\\u20602 м\\u2060/\\u2060с.\\n\\nСколько градусов в Химках?\\n\\nСейчас в Химках температура воздуха +17°. Ощущается как +15°, ясно. Температура утром +17°, днем +22°, вечером +20°, ночью +13°.\\n\\nКакая скорость и направление ветра в Химках? [...] Химки, погода сейчас: ясно, в ближайшие 2 часа осадков не ожидается. Температура воздуха +17°, ощущается как +15°. Скорость ветра 1,7 Метров в секунду, западный. Давление 744 Миллиметров ртутного столба. Влажность 47%. Температура воды днём +16°. Восход 03:53, Закат 21:16. Вчера в это время +20°\\n\\n+17°\\n\\nОщущается как +15°Вчера в это время +20°Вчера было +20°\\n\\nЯсно, в ближайшие 2 часа осадков не ожидается\\n\\n   1,7 м/с, З\\n   744\\n   47%\\n   16°\\n\\nКакой тип предупреждений вам интереснее?\\n\\nОбщий [...] Прогноз погоды на Сегодня, 5 июля: утром температура воздуха +17°, ощущается как +15°, пасмурно, скорость ветра 4 метров в секунду, западный, влажность 53%, давление 744 миллиметров ртутного столба; днём +22°, ощущается как +19°, малооблачно, скорость ветра 4,8 метров в секунду, западный, влажность 34%, давление 743 миллиметров ртутного столба; вечером +20°, ощущается как +16°, облачно с прояснениями, скорость ветра 4,5 метров в секунду, западный, влажность 36%, давление 744 миллиметров', 'score': 0.9094659}, {'title': 'Прогноз погоды в Химках (Россия) на 29 сентября (понедельник)', 'url': 'https://sinoptik.ua/ru/pohoda/khimky/2025-09-29', 'content': '# Погода в Химках 29.09.2025\\n\\nПогода в Химках\\n\\nМосковская область, г Химки (городской округ), Россия\\n\\nпонедельник\\n\\n29\\n\\nсентября\\n\\nмин.\\n\\n+5°\\n\\nмакс.\\n\\n+11°\\n\\nвторник\\n\\n30\\n\\nсентября\\n\\nмин.\\n\\n+3°\\n\\nмакс.\\n\\n+11°\\n\\nсреда\\n\\n01\\n\\nоктября\\n\\nмин.\\n\\n+3°\\n\\nмакс.\\n\\n+12°\\n\\nчетверг\\n\\n02\\n\\nоктября\\n\\nмин.\\n\\n+5°\\n\\nмакс.\\n\\n+12°\\n\\nпятница\\n\\n03\\n\\nоктября\\n\\nмин.\\n\\n+6°\\n\\nмакс.\\n\\n+13°\\n\\nсуббота\\n\\n04\\n\\nоктября\\n\\nмин.\\n\\n+7°\\n\\nмакс.\\n\\n+14°\\n\\nвоскресенье\\n\\n05\\n\\nоктября\\n\\nмин.\\n\\n+8°\\n\\nмакс.\\n\\n+11°\\n\\nпонедельник\\n\\n06\\n\\nоктября\\n\\nмин.\\n\\n+9°\\n\\nмакс.\\n\\n+11°\\n\\nвторник\\n\\n07 [...] На протяжении всего дня небо в Химках будет покрыто облаками. Без осадков.\\n\\nНародный прогноз погоды: В этот день почитается память великомученицы Евфимии Всехвальной. Если в этот день на дворе можно увидеть мн... Подробнее\\n\\n## Погода в Московской области, Россия\\n\\n+5°\\n\\n+5°\\n\\n+5°\\n\\n+5°\\n\\n+6°\\n\\n+6°\\n\\n## Погода на неделю в других городах\\n\\nХотите, чтобы на вашем сайте показывалась погода?\\n\\nПогода во всех уголках Украины, прогноз погоды от SINOPTIK [...] |  |  |  |  |  |  |  |  |\\n ---  ---  ---  --- |\\n| ночь | | утро | | день | | вечер | |\\n| 0:00 | 3:00 | 6:00 | 9:00 | 12:00 | 15:00 | 18:00 | 21:00 |\\n|  |  |  |  |  |  |  |  |\\n| +6° | +6° | +5° | +6° | +9° | +11° | +9° | +7° |\\n| +3° | +3° | +2° | +2° | +5° | +9° | +6° | +5° |\\n| 776 | 776 | 777 | 778 | 778 | 778 | 778 | 779 |\\n| 72 | 76 | 77 | 77 | 51 | 39 | 45 | 54 |\\n| 4.3 | 4.3 | 4.8 | 5.6 | 6 | 5.4 | 3.8 | 3 |\\n -  -  -  - |', 'score': 0.8757978}, {'title': 'Прогноз погоды в Химках, Московская область - Погода Mail', 'url': 'https://pogoda.mail.ru/prognoz/khimki/', 'content': 'сегодня суббота, 5 июля 2025, 21:24\\n\\n+18°\\n\\nощущается как +16°\\n\\nясно\\n\\n22:00\\n\\n+13°\\n\\n[](\\n\\n23:00\\n\\n+11°\\n\\n[](\\n\\n00:00\\n\\n+10°\\n\\n[](\\n\\n01:00\\n\\n+10°\\n\\n[](\\n\\n02:00\\n\\n+10°\\n\\n[](\\n\\n03:00\\n\\n+9°\\n\\n[](\\n\\n03:54\\n\\nвосход\\n\\n04:00\\n\\n+9°\\n\\n[](\\n\\n05:00\\n\\n+10°\\n\\n[](\\n\\n06:00\\n\\n+12°\\n\\n[](\\n\\n07:00\\n\\n+14°\\n\\n[](\\n\\n08:00\\n\\n+15°\\n\\n[](\\n\\n09:00\\n\\n+16°\\n\\n[](\\n\\n   Почасовой прогноз\\n   Метеочувствительным\\n   Погода в России\\n\\nImage 10Карта осадков\\n\\nСегодня\\n\\nВетер\\n\\n2 м/c З\\n\\nДавление\\n\\n746 мм рт.ст.\\n\\nВосход\\n\\n03:53\\n\\nЗаход\\n\\n21:16\\n\\nПочва\\n\\n+18°\\n\\nВлажность\\n\\n34% [...] +16°\\n\\nКрасногорск\\n\\n+17°\\n\\nСередниково\\n\\n+16°\\n\\nВ этом разделе вы можете узнать погоду в Химках на сегодня и завтра, а именно: изменение температуры, атмосферного давления, влажности, направление и скорость ветра, вероятность осадков, УФ-индекс и прочие погодные данные. Подробный прогноз погоды в Химках на сегодня предоставлен ИА “Метеоновости”.\\n\\nО погоде\\n\\n   Новости\\n   Сад и огород\\n\\nImage 15:  \\n\\n3 часа назад МК.RU Санкт-Петербург\\n\\n### Синоптик Шувалов предупредил петербуржцев о новом циклоне [...] Индекс УФ\\n\\n0\\n\\nГеомагнитное поле\\n\\nнеустойчивое\\n\\nФаза луны\\n\\nрастущая\\n\\nImage 11\\n\\n Продолжение после рекламы \\n\\nРеклама закроется через\\n\\nImage 12Image 13\\n\\nПолитическое убежище в США\\n\\nПоможем восстановить утраченные основания для получения убежища в США.\\n\\nПерейти\\n\\nImage 14\\n\\nРеклама\\n\\n0+\\n\\nПогода на 10 дней -----------------\\n\\n[](\\n\\nВс\\n\\n6 июля\\n\\n+22°\\n\\n+8°\\n\\nоблачно, небольшой дождь\\n\\n[](\\n\\nПн\\n\\n7 июля\\n\\n+28°\\n\\n+14°\\n\\nпеременная облачность\\n\\n[](\\n\\nВт\\n\\n8 июля\\n\\n+27°\\n\\n+18°\\n\\nпеременная облачность\\n\\n[](\\n\\nСр\\n\\n9 июля\\n\\n+30°', 'score': 0.846159}, {'title': 'Погода в Химках на 29 сентября подробно, прогноз ... - Рамблер', 'url': 'https://weather.rambler.ru/v-khimkakh/29-september/', 'content': '# Погода в Химках на 29 сентября 2025\\n\\n|  | 00:00 | 03:00 | 06:00 | 09:00 | 12:00 | 15:00 | 18:00 | 21:00 | 24:00 |\\n ---  ---  ---  ---  --- |\\n| Температура, °C | 5 | 5 | 5 | 5 | 9 | 10 | 7 | 4 | 3 |\\n| Влажность, % | 78 | 81 | 82 | 74 | 55 | 45 | 53 | 68 | 75 |\\n| Давление, мм | 758 | 759 | 759 | 760 | 760 | 759 | 759 | 760 | 760 |\\n| Ветер, м/с | 3 | 3 | 3 | 4 | 4 | 4 | 2 | 2 | 2 |\\n| Осадки, мм | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\\n\\n## Химки — погода рядом на 29 сентября 2025 [...] Благодаря нашему погодному сервису вы всегда будете в курсе, какой будет погода в Химках на 29 сентября 2025. Простые и понятные иконки и графики показывают прогноз погоды в Химках на 29 сентября 2025, в котором добавлены данные о температуре, влажности, максимальной скорости ветра и данные о других погодных явлениях. Для таких пользователей, кто любит заранее планировать свое время и быть независимым от погоды, сайт предлагает расширенный прогноз на сутки, который отражает все изменения в [...] ## Погода по городам\\n\\n### Россия\\n\\n### Мир\\n\\n### Курорты', 'score': 0.82728493}, {'title': 'Погода в Химках на 10 дней - Gismeteo', 'url': 'https://www.gismeteo.ru/weather-khimki-11582/10-days/', 'content': '2 неделиМесяцРадар\\n\\nЕщё \\n\\nНеделяГ/м активностьАрхив\\n\\nРоссия / Московская область / городской округ Химки\\n\\nПогода в Химках на 10 дней\\n\\nсб 5 июлявс 6пн 7вт 8ср 9чт 10пт 11сб 12вс 13пн 14\\n\\nТемпература воздуха,°C\\n\\n+21\\n\\n+13\\n\\n+21\\n\\n+11\\n\\n+27\\n\\n+15\\n\\n+27\\n\\n+18\\n\\n+31\\n\\n+20\\n\\n+30\\n\\n+21\\n\\n+27\\n\\n+20\\n\\n+26\\n\\n+18\\n\\n+24\\n\\n+17\\n\\n+25\\n\\n+17\\n\\nТемпература по ощущению,°C\\n\\n+21\\n\\n+13\\n\\n+21\\n\\n+11\\n\\n+27\\n\\n+15\\n\\n+27\\n\\n+18\\n\\n+31\\n\\n+21\\n\\n+30\\n\\n+21\\n\\n+27\\n\\n+20\\n\\n+26\\n\\n+20\\n\\n+24\\n\\n+18\\n\\n+25\\n\\n+17\\n\\nСреднесуточная температура,°C\\n\\n+17\\n\\n+16\\n\\n+21\\n\\n+23\\n\\n+25\\n\\n+26 [...] +23\\n\\n+22\\n\\n+21\\n\\n+21\\n\\nВетер и порывы, м/c\\n\\n2\\n\\nСЗ\\n\\n9\\n\\n1\\n\\nЮЗ\\n\\n3\\n\\n1\\n\\nЮ\\n\\n4\\n\\n2\\n\\nЮ\\n\\n5\\n\\n3\\n\\nЮВ\\n\\n10\\n\\n3\\n\\nЮВ\\n\\n7\\n\\n3\\n\\nЮВ\\n\\n9\\n\\n1\\n\\nВ\\n\\n2\\n\\n2\\n\\nВ\\n\\n3\\n\\n2\\n\\nВ\\n\\n4\\n\\nПыльца берёзы, баллы\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\nНет данных\\n\\nПыльца злаковых трав, баллы\\n\\n4\\n\\n1\\n\\n3\\n\\n2\\n\\n3\\n\\n2\\n\\n2\\n\\n1\\n\\nНет данных\\n\\nОсадки в жидком эквиваленте,мм\\n\\n0\\n\\n0,4\\n\\n0\\n\\n1,6\\n\\n0,6\\n\\n0\\n\\n0\\n\\n0\\n\\n12,9\\n\\n5,2\\n\\nВыпадающий снег, см\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\n0\\n\\nВысота снежного покрова,мм\\n\\n0 0 0 0 0 0 0 0 0 0\\n\\nДавление,мм рт.ст.\\n\\n746\\n\\n745\\n\\n746\\n\\n744\\n\\n743\\n\\n742\\n\\n742\\n\\n739\\n\\n743 [...] GISMETEO: Погода в Химках на 10 дней, прогноз погоды Химки на десять дней, городской округ Химки, Московская область, Россия\\n\\nПерейти на мобильную версию\\n\\n\\n\\nПогодаНовостиКартыПриложения\\n\\n°C\\n\\nПоиск местоположения\\n\\nПросмотренные Очистить\\n\\nБлижайшие пункты по IP-адресу\\n\\nВойдите в личный кабинет, чтобы сохранить единицы измерения, просмотренные пункты и набор данных\\n\\nVK IDЯндекс ID\\n\\nВходя в личный кабинет, вы соглашаетесь сусловиями использования\\n\\nСейчасСегодняЗавтра3 дняВыходные\\n\\n10 дней', 'score': 0.818055}]\u001b[0m\u001b[32;1m\u001b[1;3mСегодня в Химках температура воздуха составляет примерно +17°C, ощущается как +15°C.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Сегодня в Химках температура воздуха составляет примерно +17°C, ощущается как +15°C.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent with a query that should trigger the tool\n",
    "response = agent_with_searchtool.run(\"Пожалуйста, опиши сегодняшнюю погоду в Химках? Приведи температуру в градусах Цельсия\")\n",
    "\n",
    "# Output the response\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we get a much better, and real-time, response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Сегодня в Химках температура воздуха составляет примерно +17°C, ощущается как +15°C."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": [
       "Сегодня в Химках температура воздуха составляет примерно +17°C, ощущается как +15°C."
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))\n",
    "display(JSON([response]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ReAct Prompt Pattern\n",
    "\n",
    "Now that we've sketched the basics of a simple ReAct problem with a basic tool, let's look at the prompt pattern in detail. The ReAct prompt is a crucial component of the ReAct agent framework, designed to guide the language model in producing structured reasoning and action steps.\n",
    "\n",
    "#### Structure of a ReAct Prompt\n",
    "\n",
    "A ReAct prompt typically follows this structure:\n",
    "\n",
    "1. **Available Tools**: List the tools the agent can useing-action cycle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "   \n",
    "   ```python\n",
    "   You have access to the following tools:\n",
    "   1. Search: Search for information on the web.\n",
    "   2. Calculator: Perform mathematical calculations.\n",
    "   ```\n",
    "---\n",
    "\n",
    "2. **Step Instructions**: Define the expected reasoning process.\n",
    "\n",
    "--- \n",
    "   ```python\n",
    "   For each step:\n",
    "   1. Think about what you know and what you need to find out.\n",
    "   2. If you need more information, use a tool.\n",
    "   3. Format tool usage as: Action: tool_name[parameters].\n",
    "   4. After receiving an observation, continue reasoning.\n",
    "   5. When you have enough information, provide your final answer.\n",
    "   ```\n",
    "---\n",
    "\n",
    "\n",
    "3. **Example Demonstration**: Show a complete example of the reasoning-acting cycle.\n",
    "\n",
    "---   \n",
    "   ```python\n",
    "   Question: What is the population of Paris multiplied by 2?\n",
    "   Thought: I need to find the population of Paris first.\n",
    "   Action: Search[population of Paris France].\n",
    "   Observation: Paris has a population of approximately 2.16 million people.\n",
    "   Thought: Now I need to multiply this number by 2.\n",
    "   Action: Calculator[2.16 * 2]\n",
    "   Observation: 4.32\n",
    "   Answer: 4.32 million\n",
    "   ```\n",
    "---\n",
    "   \n",
    "\n",
    "4. **Current Query and Agent Scratchpad**: The actual question and a working area where the agent's thoughts, actions, and observations are recorded as it works through the problem. The scratchpad serves as the agent's memory, accumulating all steps of the reasoning process.\n",
    "\n",
    "---\n",
    "   \n",
    "   ```python\n",
    "   Question: {question}\n",
    "   {agent_scratchpad}\n",
    "   ```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function (`my_message`) that takes in a question and creates a prompt for a ReAct agent. We will include some general outputs including how the AI should behave.  Most notably the LLM was told to stop its thoughts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_message(question: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a thoughtful AI assistant tasked with solving the following question:\n",
    "\n",
    "**{question}**\n",
    "\n",
    "You will reason through the problem step by step. I will feed your previous thoughts back to you in future turns so that you can reflect and continue.\n",
    "\n",
    "Use this format for each response:\n",
    "\n",
    "---\n",
    "\n",
    "**Thought:** [your reasoning and reflection here]\n",
    "\n",
    "---\n",
    "\n",
    "Do not give a final answer yet unless explicitly asked.  \n",
    "If you see your own previous reasoning repeated as input, that means you should **continue expanding or improving** your thinking — not finalize.\n",
    "\n",
    "Only when I explicitly ask you to provide a final answer, respond with:\n",
    "\n",
    "**Final Answer:** [your answer]  \n",
    "**DONE**\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's select a question to ask our LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt =my_message('Give me a list of warm-weather vacation locations and select the best one based on other factors')\n",
    "print(my_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to incorporate  the scratchpad that serves as the agent's memory, accumulating all steps of the reasoning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing and Further Reasoning\n",
    "\n",
    "We know that incorporating observation and iterative reasoning can significantly enhance the effectiveness of an LLM. While reasoning alone enables structured thinking, acting gives the model access to real-time data or external knowledge, expanding the range of problems it can solve.\n",
    "\n",
    "However, the true boost in performance comes from the **feedback** loop—observing the result of an action and applying further reasoning based on this new information. This loop allows the LLM to refine its understanding, correct course, and approach the problem with increasing precision. This is the **Repeat or Conclude** in the ReAct framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat or Conclude\n",
    "\n",
    "The Repeat or Conclude step captures the essence of the ReAct loop — **the idea is that the LLM feeds its own output back into itself to continue reasoning**. In this example, we implement this using a 'for' loop, where the model is called multiple times, each time receiving its own previous output as part of the new input.\n",
    "\n",
    "Each time through the loop, the model produces a new reasoning trace — an intermediate thought that helps build toward the final solution. These traces are stored in a variable called **scratch_pad**, which accumulates the model’s reasoning over time and acts as a running memory.\n",
    "\n",
    "Although in this simplified example, the LLM is just reflecting on its own thoughts, this same loop structure could be extended to include tool use between iterations.\n",
    "\n",
    "Eventually, when the LLM determines that it has nothing further to add, it concludes it by outputing DONE. Here we limit the **max iterations** to four.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_pad = \"\"\n",
    "max_iterations = 4\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    display(Markdown(f\"### 🔁 Iteration {i + 1}\"))\n",
    "\n",
    "    # Initial message formatting\n",
    "    if i == 0:\n",
    "        user_input = my_message(\"Book me a vacation to a warm destination.\")\n",
    "    else:\n",
    "        user_input = scratch_pad\n",
    "\n",
    "    # Run the agent\n",
    "    response = agent_with_searchtool.run(user_input)\n",
    "\n",
    "    # Format and display the output cleanly\n",
    "    display(Markdown(f\"**📩 LLM Response:**\\n\\n```\\n{response.strip()}\\n```\"))\n",
    "\n",
    "    # Check for stopping condition\n",
    "    if \"DONE\" in response:\n",
    "        display(Markdown(\"✅ **Agent signaled DONE. Ending loop.**\"))\n",
    "        break\n",
    "\n",
    "    # Append to scratch pad\n",
    "    scratch_pad += f\"\\n\\nIteration {i + 1}:\\n{response.strip()}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(scratch_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Create a more Complex ReAct Agent from Scratch with Tools using LangGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're building a ReAct-style agent, a simple **for loop** is a great way to get started. It's easy to control, ideal for testing ideas, and allows you to feed the model’s output back into itself step by step. However, as things get more complex—for example, when it becomes **unclear** whether the output is coming from a tool or is part of the agent’s reasoning process—that simplicity can turn into a limitation.\n",
    "\n",
    "That’s where LangGraph comes in. It lets you structure your agent as a flow of reasoning steps, with built-in support for looping, memory, and a cleaner architecture.\n",
    "\n",
    "**In short**: use a 'for loop' while prototyping or learning, and switch to LangGraph when you're ready for something more scalable and maintainable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph state \n",
    "We are going to define the custom ReAct state in this example, which will contain the user's query, a list of messages, and the `agent_outcome`.\n",
    "  \n",
    "For your specific use case, feel free to add any other state keys that you need.  \n",
    "\n",
    "The `agent_outcome` field stores the result of the agent's reasoning process. This can be either an `AgentAction` (indicating the agent has decided to perform an action using a tool) or an `AgentFinish` (indicating the agent has arrived at a final answer).\n",
    "\n",
    "Including `agent_outcome` in your state allows you to:\n",
    "\n",
    "- Control Workflow: Determine the next step in your LangGraph workflow based on whether the agent has finished reasoning or needs to perform an action.\n",
    "\n",
    "- Debugging: Inspect the agent's decisions at each step, which is invaluable for debugging and understanding agent behavior.\n",
    "\n",
    "- State Management: Maintain a clear and structured state that reflects the agent's current status.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The `input` field represents the original user query or instruction. Keeping this in the state is useful for:  \n",
    "  \n",
    "- Contextual Awareness: Ensuring that the agent's actions and decisions are always grounded in the original user input.  \n",
    "\n",
    "- Logging and Auditing: Facilitating logging of user interactions for auditing or analytics purposes.  \n",
    " \n",
    "- Reusability: Allowing different parts of your workflow to access the original input without needing to pass it explicitly.\n",
    "\n",
    "---\n",
    "\n",
    "Last but not least, we need to keep track of the full conversation history between the user, the agent, and any tools that are used. This is done through the messages field.  \n",
    "  \n",
    "The messages field represents the running chat history, including:  \n",
    "\n",
    "- HumanMessage (user input)  \n",
    "  \n",
    "- AIMessage (agent's thoughts and tool calls)  \n",
    "    \n",
    "- ToolMessage (tool responses)  \n",
    "  \n",
    "- SystemMessage (instructions to the LLM) - optionally  \n",
    "  \n",
    "In LangGraph, the messages field is annotated with a **reducer**, in this case, add_messages, to allow it to be updated incrementally and intelligently.  \n",
    "\n",
    "--- \n",
    "\n",
    "**Why use add_messages as a reducer?**  \n",
    "  \n",
    "By default, LangGraph state updates **overwrite** previous values unless you use a **reducer** function. add_messages is a prebuilt smart reducer that allows:\n",
    "\n",
    "- Appending new messages during each step of the agent's execution.  \n",
    "  \n",
    "- Overwriting existing messages with the **same message ID** (e.g., retrying or correcting a tool call).  \n",
    "  \n",
    "- Human-in-the-loop editing, where a user or external process can revise the conversation history without causing duplication.  \n",
    "  \n",
    "This is more powerful and safer than using operator.add, which simply appends new items blindly (potentially duplicating entries).  \n",
    "  \n",
    "Together, these three fields — input, agent_outcome, and messages — provide a robust foundation for any ReAct-style agent built with LangGraph. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict, Union, Sequence, List, Tuple\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    This defines the structure of our agent's state:\n",
    "    - input: The user's query or instruction\n",
    "    - agent_outcome: What the agent decides to do (take an action or provide final answer)\n",
    "    - messages: The conversation history\n",
    "    - intermediate_steps: A history of (action, observation) pairs used for scratchpad prompting\n",
    "    \"\"\"\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    \n",
    "    input: str\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all the Tools for the ReAct Agent\n",
    "\n",
    "We had defined the `search tool` before, but to build a more complex ReAct agent we require  more tools for the agent to choose from during its action phase. Since the ReAct pattern emphasizes reasoning before acting, we need to define these tools upfront so the agent can reason about which tools to use and how to use them effectively.  \n",
    "\n",
    "Here, we also define a `recommend_clothing` tool. The agent will use it when asked to recommend proper clothing for a weather. This is just an example. You can add as many tools as your agent required here. Make sure to include what the tool does inside the docstrings under tool definition so the agent can understand what the tool does and when to use it, for example :  \n",
    "\n",
    "    \"\"\"\n",
    "    Suggest what to wear based on weather conditions.\n",
    "\n",
    "    :param weather: A brief description of the weather (e.g. \"Overcast, 64.9°F\")\n",
    "    :return: A recommendation on what to wear\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Search Integration\n",
    "\n",
    "Let's set up the Tavily search tool that will allow our agent to perform external research.\n",
    "  \n",
    "This code sets up a web search tool using the Tavily API:\n",
    "\n",
    "`TavilySearchAPIWrapper()` creates a basic search interface that:\n",
    "* Makes HTTP requests to Tavily's API endpoints.\n",
    "* Handles authentication using your API key.\n",
    "* Processes raw search results into readable text.\n",
    "* Returns plaintext summaries of search results.\n",
    "\n",
    "The output is a dictionary where:\n",
    "* `url`: Contains the source webpage URL.\n",
    "* `content`: Contains the relevant text excerpt from the webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key Setup\n",
    "\n",
    "We'll use Tavily search as our external research tool. You can get an API key at https://app.tavily.com/sign-in    \n",
    "\n",
    "**Disclaimer:** Signing up for Tavily provides you with free credits, more than enough for this project's needs. If you require additional credits for further use, please add them at your own discretion.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the tools\n",
    "This function uses a web search engine (like Tavily or DuckDuckGo) to retrieve and return relevant results for a given query string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External tools\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import tool\n",
    "import os\n",
    "\n",
    "search = TavilySearchResults()\n",
    "\n",
    "# Search Tool\n",
    "@tool\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for information using a search engine like Tavily or DuckDuckGo.\n",
    "\n",
    "    :param query: The search query string\n",
    "    :return: Search results related to the query\n",
    "    \"\"\"\n",
    "    return search.invoke(query) \n",
    "\n",
    "\n",
    "# Note: This tool connects our agent to Tavily’s real-time search API, enabling retrieval\n",
    "# of up-to-date web content. Ideal for queries requiring current information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function analyzes a weather description string and returns a clothing recommendation by checking for specific keywords or temperatures—for example, if it finds words such as 'snow' or 'freezing', it suggests wearing warm clothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def recommend_clothing(weather: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a clothing recommendation based on the provided weather description.\n",
    "\n",
    "    This function examines the input string for specific keywords or temperature indicators \n",
    "    (e.g., \"snow\", \"freezing\", \"rain\", \"85°F\") to suggest appropriate attire. It handles \n",
    "    common weather conditions like snow, rain, heat, and cold by providing simple and practical \n",
    "    clothing advice.\n",
    "\n",
    "    :param weather: A brief description of the weather (e.g., \"Overcast, 64.9°F\")\n",
    "    :return: A string with clothing recommendations suitable for the weather\n",
    "    \"\"\"\n",
    "    weather = weather.lower()\n",
    "    if \"snow\" in weather or \"freezing\" in weather:\n",
    "        return \"Wear a heavy coat, gloves, and boots.\"\n",
    "    elif \"rain\" in weather or \"wet\" in weather:\n",
    "        return \"Bring a raincoat and waterproof shoes.\"\n",
    "    elif \"hot\" in weather or \"85\" in weather:\n",
    "        return \"T-shirt, shorts, and sunscreen recommended.\"\n",
    "    elif \"cold\" in weather or \"50\" in weather:\n",
    "        return \"Wear a warm jacket or sweater.\"\n",
    "    else:\n",
    "        return \"A light jacket should be fine.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools can be added to a list to be used as input into the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool, recommend_clothing]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent will also need the tool names, they are extracted using: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "print(tools_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "Next, we will define the tools and model we will use for our graph.   \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ReAct Prompt Template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the ReAct Prompt from the Hub\n",
    "\n",
    "Now that we have our tools and language model set up, the next step is to obtain the ReAct prompt template. This prompt guides the language model to follow the Reasoning–Acting–Observing pattern that defines the ReAct approach. You'll notice that it's similar to the prompt we discussed earlier.  \n",
    "\n",
    "The prompt must have input keys:  \n",
    "            - `tools`: contains descriptions and arguments for each tool.  \n",
    "            -`tool_names`: contains all tool names.  \n",
    "            -`agent_scratchpad`: contains previous agent actions and tool outputs as a string.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "print(react_prompt)  # this shows the full react_prompt\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"📄 ReAct Prompt Template\")\n",
    "print(\"=\" * 40)\n",
    "print(react_prompt.template)\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hwchase17/react` prompt is a well-designed template specifically crafted for ReAct agents. It contains the necessary structure to:\n",
    "- Guide the model to think through problems step by step.\n",
    "- Format its actions to invoke the appropriate tools.\n",
    "- Process observations from the tool calls.\n",
    "- Make decisions based on accumulated information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Load ReAct prompt from LangChain Hub\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "\n",
    "def format_intermediate_steps(intermediate_steps):\n",
    "    \"\"\"Convert intermediate steps into a string for the prompt scratchpad.\"\"\"\n",
    "    scratchpad = \"\"\n",
    "    for action, observation in intermediate_steps:\n",
    "        scratchpad += f\"Thought: {action.log}\\n\"\n",
    "        scratchpad += f\"Action: {action.tool}\\n\"\n",
    "        scratchpad += f\"Action Input: {action.tool_input}\\n\"\n",
    "        scratchpad += f\"Observation: {observation}\\n\"\n",
    "    return scratchpad\n",
    "\n",
    "\n",
    "\n",
    "def call_model(state: AgentState, config: RunnableConfig):\n",
    "    # We have to format tool names as a string for the prompt (e.g., \"search_tool, recommend_clothing\")\n",
    "    # Get tools from config\n",
    "    tools = config[\"configurable\"].get(\"tools\", []) # configurable is an Arbitrary dictionary you define (e.g., tool list)\n",
    "    # .get() won’t raise an error if the key doesn’t exist — it just returns a fallback instead, [] in this case. \n",
    "    \n",
    "\n",
    "    # Prepare tool names for prompt\n",
    "    tool_names = \", \".join(tool.name for tool in tools)\n",
    "    \n",
    "    \n",
    "    # Format the scratchpad (past reasoning steps)\n",
    "    scratchpad = format_intermediate_steps(state.get(\"intermediate_steps\", []))\n",
    "\n",
    "    # Format the ReAct-style prompt\n",
    "    formatted_prompt = react_prompt.format(\n",
    "        input=state[\"input\"],\n",
    "        tools=tool_names,\n",
    "        tool_names=tool_names,\n",
    "        agent_scratchpad=scratchpad\n",
    "    )\n",
    "\n",
    "    # Use it as a HumanMessage to feed into the model\n",
    "    # giving the LLM a formatted string that includes the question and context — \n",
    "    # i.e., what the \"user\" might say and how the model should behave in response.\n",
    "    prompt_message = HumanMessage(content=formatted_prompt) \n",
    "\n",
    "    # Call the model with prompt only (message history is embedded in the prompt text)\n",
    "    response: AIMessage = model.invoke([prompt_message], config)\n",
    "  \n",
    "\n",
    "    # Determine agent outcome\n",
    "    if response.tool_calls:\n",
    "        agent_outcome = AgentAction(\n",
    "            tool=response.tool_calls[0][\"name\"],\n",
    "            tool_input=response.tool_calls[0][\"args\"],\n",
    "            log=response.content or \"\",\n",
    "        )\n",
    "    else:\n",
    "        agent_outcome = AgentFinish(\n",
    "            return_values={\"output\": response.content},\n",
    "            log=\"Agent has determined the final answer.\",\n",
    "        )\n",
    "        print(\"we ard done\",response.content)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"agent_outcome\": agent_outcome,\n",
    "        \"agent_scratchpad\": scratchpad\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Define the user query and initial state\n",
    "user_query = \"What’s the weather in Zurich, and what should I wear check the temperture?\"\n",
    "\n",
    "fist_input = {\n",
    "    \"input\": user_query,\n",
    "    \"messages\": [HumanMessage(content=user_query)],\n",
    "    \"intermediate_steps\": []  # this is mandatory for scratchpad to work\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratchpad = format_intermediate_steps(fist_input.get(\"intermediate_steps\", []))\n",
    "scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = \", \".join(tool.name for tool in tools)\n",
    "tool_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = react_prompt.format(\n",
    "        input=fist_input [\"input\"],\n",
    "        tools=tool_names,\n",
    "        tool_names=tool_names,\n",
    "        agent_scratchpad=scratchpad\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph injects the tools into each node's function call, and if the node is calling a model (such as your call_model), it also wraps the model with those tools using .with_config(...) behind the scenes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tool = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: AIMessage =model_tool.invoke([HumanMessage(formatted_prompt)])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nodes and edges  \n",
    "Next, let's define our nodes and edges.  \n",
    "  \n",
    "Each node should:\n",
    "\n",
    "- Preserve and update messages.\n",
    "- Set agent_outcome based on the model's decision (optional). \n",
    "- Keep input untouched.\n",
    "    \n",
    "You might consider adding a new node to produce structured outputs or trigger external actions like advanced calculations, or creating a calendar event. Or perhaps you want to modify how the `call_model` node behaves, or adjust the logic in `should_continue` to decide when tools should be used. With LangGraph, it's easy to tailor this foundational structure to fit your unique workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "#Define our tool node \n",
    "\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    outputs = []\n",
    "    new_steps = []\n",
    "\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "\n",
    "        # Append ToolMessage to message list\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add intermediate step\n",
    "        new_steps.append((\n",
    "            AgentAction(\n",
    "                tool=tool_call[\"name\"],\n",
    "                tool_input=tool_call[\"args\"],\n",
    "                log=state[\"messages\"][-1].content or \"\",\n",
    "            ),\n",
    "            str(tool_result)\n",
    "        ))\n",
    "\n",
    "    return {\n",
    "        \"messages\": outputs,\n",
    "        \"intermediate_steps\": new_steps  # 👈 critical!\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    if isinstance(state[\"agent_outcome\"], AgentFinish):\n",
    "        return \"end\"\n",
    "    return \"continue\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph  \n",
    "Now that we have defined all of our nodes and edges, we can define and compile our graph. Depending on if you have added more nodes or different edges, you will need to edit this to fit your specific use case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"reason\", call_model)\n",
    "workflow.add_node(\"act\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"reason\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `reason`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"reason\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `act`, then we call the tool node.\n",
    "        \"continue\": \"act\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"act\", \"reason\")\n",
    "\n",
    "# Now we can compile and visualize our graph\n",
    "graph = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ReAct agent  \n",
    "Now that we have created our ReAct agent, let's actually put it to the test!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample execution of the message-based ReAct agent using LangGraph #working version\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 Initializing ReAct Agent with LangGraph\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Define the user query and initial state\n",
    "    user_query = \"What’s the weather in Zurich, and what should I wear?\"\n",
    "\n",
    "    inputs = {\n",
    "    \"input\": user_query,\n",
    "    \"messages\": [HumanMessage(content=user_query)],\n",
    "    \"intermediate_steps\": []  # this is mandatory for scratchpad to work\n",
    "}\n",
    "\n",
    "    # Run the agent graph synchronously\n",
    "    final_state = graph.invoke(\n",
    "    inputs,\n",
    "    config={\"configurable\": {\"tools\": tools}}  #  Pass tools here\n",
    "    )\n",
    "    #final_state = graph.invoke(inputs)\n",
    "\n",
    "    # Extract and display the final answer (if available)\n",
    "    print(\"\\n✅ Final Answer:\")\n",
    "    finish = final_state.get(\"agent_outcome\")\n",
    "    if isinstance(finish, AgentFinish):\n",
    "        print(finish.return_values[\"output\"])\n",
    "    else:\n",
    "        print(\"No final answer was returned.\")\n",
    "\n",
    "    # Print the full reasoning trace from messages\n",
    "    print(\"\\n🔎 Reasoning Trace:\")\n",
    "    for msg in final_state[\"messages\"]:\n",
    "        try:\n",
    "            msg.pretty_print()\n",
    "        except AttributeError:\n",
    "            print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Agent Execution Summary\n",
    "\n",
    "### Flow Overview\n",
    "The agent successfully executed a multi-step reasoning task using LangGraph and ReAct prompting.\n",
    "Below is a breakdown of the workflow:\n",
    "\n",
    "### Input\n",
    "**User query:**  \n",
    "*\"What’s the weather in Zurich, and what should I wear?\"*\n",
    "\n",
    "### Step-by-Step Reasoning\n",
    "\n",
    "1. **First tool call:**  \n",
    "   - **Tool:** `search_tool`  \n",
    "   - **Input:** `\"Current weather in Zurich\"`  \n",
    "   - **Observation:** `\"The current weather in Zurich is 10 degrees Celsius with light rain.\"`\n",
    "\n",
    "2. **Second tool call:**  \n",
    "   - **Tool:** `recommend_clothing`  \n",
    "   - **Input:** `\"10 degrees Celsius, light rain\"`  \n",
    "   - **Observation:** `\"For 10 degrees Celsius with light rain, it is recommended to wear a medium-weight jacket, \n",
    "      a sweater, long pants, and waterproof shoes. An umbrella or a raincoat would also be useful.\"`\n",
    "\n",
    "3. **Final reasoning and answer:**\n",
    "   - The agent concluded its thought process.\n",
    "   - **Final Answer:**  \n",
    "     *\"The current weather in Zurich is 10 degrees Celsius with light rain. It is recommended to wear a medium-weight \n",
    "     jacket, a sweater, long pants, and waterproof shoes. An umbrella or a raincoat would also be useful.\"*\n",
    "\n",
    "### System Behavior\n",
    "\n",
    "| Component               | Description                                                                                   | Status |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------|--------|\n",
    "| Input message           | Parsed correctly and injected into initial state                                              | ✅ Passed |\n",
    "| First tool execution    | Weather data retrieved from search tool                                                      | ✅ Passed |\n",
    "| Second tool execution   | Clothing advice generated based on weather                                                   | ✅ Passed |\n",
    "| Scratchpad generation   | Thought, action, input, and observation logs formatted into a coherent history string         | ✅ Passed |\n",
    "| Agent outcome tracking  | `AgentAction` and `AgentFinish` returned and handled appropriately                            | ✅ Passed |\n",
    "| Reasoning trace output  | All intermediate steps and final result printed in readable format                           | ✅ Passed |\n",
    "\n",
    "This confirms a correct and fully functional ReAct implementation using LangGraph\n",
    "with tool chaining and reasoning history tracking. You can also define additional tools such as translation or summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - add a tool \n",
    "\n",
    "Add a temperature conversion tool to convert from Fahrenheit to Celcius and vice versa. Then test your ReAct agent with the following prompt:  \n",
    "\"What’s the weather in Tokyo today, what should I wear, and can you convert the temperature to Fahrenheit?\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "@tool\n",
    "def convert_temperature(temperature: float, unit: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts temperature between Celsius and Fahrenheit.\n",
    "    \n",
    "    :param temperature: The numeric temperature to convert.\n",
    "    :param unit: 'C' to convert to Celsius, 'F' to convert to Fahrenheit.\n",
    "    :return: Converted temperature as a string.\n",
    "    \"\"\"\n",
    "    unit = unit.upper()\n",
    "    if unit == \"C\":\n",
    "        converted = (temperature - 32) * 5 / 9\n",
    "        return f\"{temperature}°F is {converted:.1f}°C\"\n",
    "    elif unit == \"F\":\n",
    "        converted = (temperature * 9 / 5) + 32\n",
    "        return f\"{temperature}°C is {converted:.1f}°F\"\n",
    "    else:\n",
    "        return \"Invalid unit. Please use 'C' or 'F'.\"\n",
    "\n",
    "tools.append(convert_temperature)\n",
    "tools_by_name[convert_temperature.name] = convert_temperature\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\n",
    "\n",
    "- https://www.ibm.com/think/topics/react-agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Faranak Heidari](https://www.linkedin.com/in/faranakhdr/) is a data scientist and GenAI developer in IBM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kunal Makwana is a software developer in IBM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for the changelog</summary>\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2025-06-24|0.4|Mercedes Schneider|QA pass with edits|\n",
    "|2025-06-24|0.3|Steve Ryan|ID review and format/typo fixes|\n",
    "|2024-02-23|0.2|Elio Di Nino|Update library documentation|\n",
    "|2020-07-17|0.1|Sam|Create lab template|\n",
    "\n",
    "</detials>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "9b7e5cef7486e12ef43928838fe7844077fc76200d514eb7d71f5f56d635076a"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
